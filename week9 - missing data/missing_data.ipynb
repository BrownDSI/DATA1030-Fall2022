{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mud card answers\n",
    "- **How to choose the better model for the test dataset if model A has a slightly lower mean value than model B but has a slightly larger standard deviation value than model B?**\n",
    "    - good question!\n",
    "    - I'd probably still chose the model with a slightly higher mean\n",
    "- **What if our data is a time series data and we split it in a non-random way? They there would be no point in going through multiple random states?**\n",
    "    - the uncertainty in the test score comes from two sources:\n",
    "        - splitting\n",
    "        - ML model\n",
    "    - the only way to have 0 uncertainty if both the splitting and the ML model are both deterministic\n",
    "- **Once we identify the optimal hyper parameters using the train and validation sets, what data do we use to train our final model that calculates generalization error via the test set? Do we go back and train a model with all data from train and validation together? Or do we just use the model fit to only train data that generated the optimal greatest score?**\n",
    "    - good question!\n",
    "    - you can retrain the best model using the train and val sets, I don't see a problem with that\n",
    "- **\"When setting the params dictionary for GridSearchCV- we have to specify the keys as modelType__hyperparameter name. In examples online I didn't find the correct names for model_Type... are these in SKlearn? e.g, randomforestclassifier__gamma**\n",
    "    - whatever the name of the ML model is, that's what model_Type should be. E.g., lasso, logisticregression, randomforestclassifier, svc, svr\n",
    "- **So we should test out multiple different models and tune the hyperparameters for each?**\n",
    "    - yes\n",
    "- **Is there a preference for the use of Tensorflow or sci kit learn models?**\n",
    "    - we don't do deep learning in this course so use sklearn\n",
    "    - you will use keras and tensorflow in CSCI2470\n",
    "- **Can SVC and RandomForest handle missing data? If not, what kind of imputation should be performed for pre-processing?**\n",
    "    - no, sklearn doesn't natively support missing values\n",
    "    - we will cover this today\n",
    "    - no need to impute during preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Missing data\n",
    "\n",
    "By the end of this module, you will be able to\n",
    "- apply multivariate imputation\n",
    "- apply XGBoost to a dataset with missing values\n",
    "- apply the reduced-features model (also called the pattern submodel approach)\n",
    "- decide which approach is best for your dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## We continue working with the house price data set\n",
    "- regression problem\n",
    "- categorical, ordinal, continuous features\n",
    "- missing data in all feature types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 79)\n"
     ]
    }
   ],
   "source": [
    "# read the data\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Let's load the data\n",
    "df = pd.read_csv('data/train.csv')\n",
    "# drop the ID\n",
    "df.drop(columns=['Id'],inplace=True)\n",
    "\n",
    "# the target variable\n",
    "y = df['SalePrice']\n",
    "df.drop(columns=['SalePrice'],inplace=True)\n",
    "# the unprocessed feature matrix\n",
    "X = df.values\n",
    "print(X.shape)\n",
    "# the feature names\n",
    "ftrs = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fraction of missing values in features:\n",
      "LotFrontage     0.177397\n",
      "Alley           0.937671\n",
      "MasVnrType      0.005479\n",
      "MasVnrArea      0.005479\n",
      "BsmtQual        0.025342\n",
      "BsmtCond        0.025342\n",
      "BsmtExposure    0.026027\n",
      "BsmtFinType1    0.025342\n",
      "BsmtFinType2    0.026027\n",
      "Electrical      0.000685\n",
      "FireplaceQu     0.472603\n",
      "GarageType      0.055479\n",
      "GarageYrBlt     0.055479\n",
      "GarageFinish    0.055479\n",
      "GarageQual      0.055479\n",
      "GarageCond      0.055479\n",
      "PoolQC          0.995205\n",
      "Fence           0.807534\n",
      "MiscFeature     0.963014\n",
      "dtype: float64\n",
      "data types of the features with missing values:\n",
      "LotFrontage     float64\n",
      "Alley            object\n",
      "MasVnrType       object\n",
      "MasVnrArea      float64\n",
      "BsmtQual         object\n",
      "BsmtCond         object\n",
      "BsmtExposure     object\n",
      "BsmtFinType1     object\n",
      "BsmtFinType2     object\n",
      "Electrical       object\n",
      "FireplaceQu      object\n",
      "GarageType       object\n",
      "GarageYrBlt     float64\n",
      "GarageFinish     object\n",
      "GarageQual       object\n",
      "GarageCond       object\n",
      "PoolQC           object\n",
      "Fence            object\n",
      "MiscFeature      object\n",
      "dtype: object\n",
      "fraction of points with missing values: 1.0\n"
     ]
    }
   ],
   "source": [
    "perc_missing_per_ftr = df.isnull().sum(axis=0)/df.shape[0]\n",
    "print('fraction of missing values in features:')\n",
    "print(perc_missing_per_ftr[perc_missing_per_ftr > 0])\n",
    "print('data types of the features with missing values:')\n",
    "print(df[perc_missing_per_ftr[perc_missing_per_ftr > 0].index].dtypes)\n",
    "frac_missing = sum(df.isnull().sum(axis=1)!=0)/df.shape[0]\n",
    "print('fraction of points with missing values:',frac_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(876, 79)\n",
      "(292, 79)\n",
      "(292, 79)\n"
     ]
    }
   ],
   "source": [
    "# let's split to train, CV, and test\n",
    "X_other, X_test, y_other, y_test = train_test_split(df, y, test_size=0.2, random_state=0)\n",
    "X_train, X_CV, y_train, y_CV = train_test_split(X_other, y_other, test_size=0.25, random_state=0)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_CV.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# collect the various features\n",
    "cat_ftrs = ['MSZoning','Street','Alley','LandContour','LotConfig','Neighborhood','Condition1','Condition2',\\\n",
    "            'BldgType','HouseStyle','RoofStyle','RoofMatl','Exterior1st','Exterior2nd','MasVnrType','Foundation',\\\n",
    "           'Heating','CentralAir','Electrical','GarageType','PavedDrive','MiscFeature','SaleType','SaleCondition']\n",
    "ordinal_ftrs = ['LotShape','Utilities','LandSlope','ExterQual','ExterCond','BsmtQual','BsmtCond','BsmtExposure',\\\n",
    "               'BsmtFinType1','BsmtFinType2','HeatingQC','KitchenQual','Functional','FireplaceQu','GarageFinish',\\\n",
    "               'GarageQual','GarageCond','PoolQC','Fence']\n",
    "ordinal_cats = [['Reg','IR1','IR2','IR3'],['AllPub','NoSewr','NoSeWa','ELO'],['Gtl','Mod','Sev'],\\\n",
    "               ['Po','Fa','TA','Gd','Ex'],['Po','Fa','TA','Gd','Ex'],['NA','Po','Fa','TA','Gd','Ex'],\\\n",
    "               ['NA','Po','Fa','TA','Gd','Ex'],['NA','No','Mn','Av','Gd'],['NA','Unf','LwQ','Rec','BLQ','ALQ','GLQ'],\\\n",
    "               ['NA','Unf','LwQ','Rec','BLQ','ALQ','GLQ'],['Po','Fa','TA','Gd','Ex'],['Po','Fa','TA','Gd','Ex'],\\\n",
    "               ['Sal','Sev','Maj2','Maj1','Mod','Min2','Min1','Typ'],['NA','Po','Fa','TA','Gd','Ex'],\\\n",
    "               ['NA','Unf','RFn','Fin'],['NA','Po','Fa','TA','Gd','Ex'],['NA','Po','Fa','TA','Gd','Ex'],\n",
    "               ['NA','Fa','TA','Gd','Ex'],['NA','MnWw','GdWo','MnPrv','GdPrv']]\n",
    "num_ftrs = ['MSSubClass','LotFrontage','LotArea','OverallQual','OverallCond','YearBuilt','YearRemodAdd',\\\n",
    "             'MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','1stFlrSF','2ndFlrSF',\\\n",
    "             'LowQualFinSF','GrLivArea','BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr',\\\n",
    "             'KitchenAbvGr','TotRmsAbvGrd','Fireplaces','GarageYrBlt','GarageCars','GarageArea','WoodDeckSF',\\\n",
    "             'OpenPorchSF','EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','MiscVal','MoSold','YrSold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# preprocess with pipeline and columntransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# one-hot encoder\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant',fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(sparse=False,handle_unknown='ignore'))])\n",
    "\n",
    "# ordinal encoder\n",
    "ordinal_transformer = Pipeline(steps=[\n",
    "    ('imputer2', SimpleImputer(strategy='constant',fill_value='NA')),\n",
    "    ('ordinal', OrdinalEncoder(categories = ordinal_cats))])\n",
    "\n",
    "# standard scaler\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "# collect the transformers\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, num_ftrs),\n",
    "        ('cat', categorical_transformer, cat_ftrs),\n",
    "        ('ord', ordinal_transformer, ordinal_ftrs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(876, 221)\n",
      "(292, 221)\n",
      "(292, 221)\n"
     ]
    }
   ],
   "source": [
    "# fit_transform the training set\n",
    "X_prep = preprocessor.fit_transform(X_train)\n",
    "# collect feature names\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "\n",
    "df_train = pd.DataFrame(data=X_prep,columns=feature_names)\n",
    "print(df_train.shape)\n",
    "\n",
    "# transform the CV\n",
    "df_CV = preprocessor.transform(X_CV)\n",
    "df_CV = pd.DataFrame(data=df_CV,columns = feature_names)\n",
    "print(df_CV.shape)\n",
    "\n",
    "# transform the test\n",
    "df_test = preprocessor.transform(X_test)\n",
    "df_test = pd.DataFrame(data=df_test,columns = feature_names)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data dimensions: (876, 221)\n",
      "fraction of missing values in features:\n",
      "num__LotFrontage    0.173516\n",
      "num__MasVnrArea     0.004566\n",
      "num__GarageYrBlt    0.050228\n",
      "dtype: float64\n",
      "fraction of points with missing values: 0.2237442922374429\n"
     ]
    }
   ],
   "source": [
    "print('data dimensions:',df_train.shape)\n",
    "perc_missing_per_ftr = df_train.isnull().sum(axis=0)/df_train.shape[0]\n",
    "print('fraction of missing values in features:')\n",
    "print(perc_missing_per_ftr[perc_missing_per_ftr > 0])\n",
    "frac_missing = sum(df_train.isnull().sum(axis=1)!=0)/df_train.shape[0]\n",
    "print('fraction of points with missing values:',frac_missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color='LIGHTGRAY'>Missing data</font>\n",
    "\n",
    "<font color='LIGHTGRAY'>By the end of this module, you will be able to</font>\n",
    "- **apply multivariate imputation**\n",
    "- <font color='LIGHTGRAY'>apply XGBoost to a dataset with missing values</font>\n",
    "- <font color='LIGHTGRAY'>apply the reduced-features model (also called the pattern submodel approach)</font>\n",
    "- <font color='LIGHTGRAY'>decide which approach is best for your dataset</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multivariate Imputation\n",
    "\n",
    "- models each feature with missing values as a function of other features, and uses that estimate for imputation\n",
    "   - at each step, a feature is designated as target variable and the other feature columns are treated as feature matrix X\n",
    "   - a regressor is trained on (X, y) for known y\n",
    "   - then, the regressor is used to predict the missing values of y\n",
    "- in the ML pipeline:\n",
    "   - create n imputed datasets\n",
    "   - run all of them through the ML pipeline\n",
    "   - generate n test scores\n",
    "   - the uncertainty in the test scores is due to the uncertainty in imputation\n",
    "- paper [here](https://www.jstatsoft.org/article/view/v045i03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### sklearn's IterativeImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   num__LotFrontage  num__MasVnrArea  num__GarageYrBlt\n",
      "0          0.424926        -0.573303          0.979398\n",
      "1               NaN         0.492835          1.018748\n",
      "2               NaN        -0.573303          0.192399\n",
      "3         -0.049970         0.810076         -0.476551\n",
      "4         -1.474659        -0.022031          0.979398\n",
      "   num__LotFrontage  num__MasVnrArea  num__GarageYrBlt\n",
      "0          0.424926        -0.573303          0.979398\n",
      "1         -1.172453         0.492835          1.018748\n",
      "2         -0.568039        -0.573303          0.192399\n",
      "3         -0.049970         0.810076         -0.476551\n",
      "4         -1.474659        -0.022031          0.979398\n"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "print(df_train[['num__LotFrontage','num__MasVnrArea','num__GarageYrBlt']].head())\n",
    "\n",
    "imputer = IterativeImputer(estimator = RandomForestRegressor(n_estimators=1), random_state=42)\n",
    "X_impute = imputer.fit_transform(df_train)\n",
    "df_train_imp = pd.DataFrame(data=X_impute, columns = df_train.columns)\n",
    "\n",
    "print(df_train_imp[['num__LotFrontage','num__MasVnrArea','num__GarageYrBlt']].head())\n",
    "\n",
    "df_CV_imp = pd.DataFrame(data=imputer.transform(df_CV), columns = df_train.columns)\n",
    "df_test_imp = pd.DataFrame(data=imputer.transform(df_test), columns = df_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does it make sense to impute?\n",
    "- GarageYearBuilt should definitely not be imputed because a missing value indicates no garage on the property"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color='LIGHTGRAY'>Missing data</font>\n",
    "\n",
    "<font color='LIGHTGRAY'>By the end of this module, you will be able to</font>\n",
    "- <font color='LIGHTGRAY'>apply multivariate imputation</font>\n",
    "- **apply XGBoost to a dataset with missing values**\n",
    "- <font color='LIGHTGRAY'>apply the reduced-features model (also called the pattern submodel approach)</font>\n",
    "- <font color='LIGHTGRAY'>decide which approach is best for your dataset</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## XGBoost\n",
    "- eXtreme Gradient Boosting - a popular tree-based method\n",
    "- [blog post](https://xgboost.readthedocs.io/en/latest/tutorials/model.html) and [paper](http://delivery.acm.org/10.1145/2940000/2939785/p785-chen.pdf)\n",
    "- more advanced than random forest\n",
    "   - it has l1 and l2 regularization while random forest does not\n",
    "   - trees are not independent\n",
    "      - the next tree is built to improve the previous tree\n",
    "      - less trees are necessary to achieve same accuracy\n",
    "      - but XGBoost trees can overfit - more on this in the problem set\n",
    "   - handles missing values well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## XGBoost and missing values\n",
    "- sklearn raises an error if the feature matrix (X) contains nans. \n",
    "- XGBoost doesn't! \n",
    "- If a feature with missing values is split:\n",
    "    - XGBoost tries to put the points with missing values to the left and right\n",
    "    - calculates the impurity measure for both options\n",
    "    - puts the points with missing values to the side with the lower impurity\n",
    "- if missingness correlates with the target variable, XGBoost extracts this info!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the CV RMSE: 23470.132687324658\n",
      "the test RMSE: 31748.96283078089\n",
      "the test R2: 0.8540372805542484\n"
     ]
    }
   ],
   "source": [
    "import xgboost\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "param_grid = {\"learning_rate\": [0.03],\n",
    "              \"n_estimators\": [10000],\n",
    "              \"seed\": [0],\n",
    "              #\"reg_alpha\": [0e0, 1e-2, 1e-1, 1e0, 1e1, 1e2],\n",
    "              #\"reg_lambda\": [0e0, 1e-2, 1e-1, 1e0, 1e1, 1e2],\n",
    "              \"missing\": [np.nan], \n",
    "              #\"max_depth\": [1,3,10,30,100],\n",
    "              \"colsample_bytree\": [0.9],              \n",
    "              \"subsample\": [0.66]}\n",
    "\n",
    "XGB = xgboost.XGBRegressor()\n",
    "XGB.set_params(**ParameterGrid(param_grid)[0]) # ONLY THE ONE MODEL IS TRAINED HERE!\n",
    "XGB.fit(df_train,y_train,early_stopping_rounds=50,eval_set=[(df_CV, y_CV)], verbose=False)\n",
    "y_CV_pred = XGB.predict(df_CV)\n",
    "print('the CV RMSE:',np.sqrt(mean_squared_error(y_CV,y_CV_pred)))\n",
    "y_test_pred = XGB.predict(df_test)\n",
    "print('the test RMSE:',np.sqrt(mean_squared_error(y_test,y_test_pred)))\n",
    "print('the test R2:',r2_score(y_test,y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### XGBoost with the imputed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the CV RMSE: 23307.742588617937\n",
      "the test RMSE: 33077.233938561745\n",
      "the test R2: 0.8415686105650563\n"
     ]
    }
   ],
   "source": [
    "XGB.fit(df_train_imp,y_train,early_stopping_rounds=50,eval_set=[(df_CV_imp, y_CV)], verbose=False)\n",
    "y_CV_pred = XGB.predict(df_CV_imp)\n",
    "print('the CV RMSE:',np.sqrt(mean_squared_error(y_CV,y_CV_pred)))\n",
    "y_test_pred = XGB.predict(df_test_imp)\n",
    "print('the test RMSE:',np.sqrt(mean_squared_error(y_test,y_test_pred)))\n",
    "print('the test R2:',r2_score(y_test,y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='LIGHTGRAY'>Missing data</font>\n",
    "\n",
    "<font color='LIGHTGRAY'>By the end of this module, you will be able to</font>\n",
    "- <font color='LIGHTGRAY'>apply multivariate imputation</font>\n",
    "- <font color='LIGHTGRAY'>apply XGBoost to a dataset with missing values</font>\n",
    "- **apply the reduced-features model (also called the pattern submodel approach)**\n",
    "- <font color='LIGHTGRAY'>decide which approach is best for your dataset</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reduced-features model (or pattern submodel approach)\n",
    "- first described in 2007 in a [JMLR article](http://www.jmlr.org/papers/v8/saar-tsechansky07a.html) as the reduced features model\n",
    "- in 2018, \"rediscovered\" as the pattern submodel approach in [Biostatistics](https://www.ncbi.nlm.nih.gov/pubmed/30203058)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>My test set:</center>\n",
    "\n",
    "| index \t| feature 1 \t| feature 2 \t| feature 3 \t| target var \t|\n",
    "|-------\t|:---------:\t|:---------:\t|:---------:\t|:----------:\t|\n",
    "| 0     \t|     <font color='red'>NA</font>    \t|     45    \t|     <font color='red'>NA</font>    \t|      0     \t|\n",
    "| 1     \t|     <font color='red'>NA</font>    \t|     <font color='red'>NA</font>    \t|     8     \t|      1     \t|\n",
    "| 2     \t|     12    \t|     6     \t|     34    \t|      0     \t|\n",
    "| 3     \t|     1     \t|     89    \t|     <font color='red'>NA</font>    \t|      0     \t|\n",
    "| 4     \t|     0     \t|     <font color='red'>NA</font>    \t|     47    \t|      1     \t|\n",
    "| 5     \t|    687    \t|     24    \t|     67    \t|      1     \t|\n",
    "| 6     \t|     <font color='red'>NA</font>    \t|     23    \t|     <font color='red'>NA</font>    \t|      1     \t|\n",
    "\n",
    "To predict points 0 and 6, I will use train and CV points that are complete in feature 2.\n",
    "\n",
    "To predict point 1, I will use train and CV points that are complete in feature 3.\n",
    "\n",
    "To predict point 2 and 5, I will use train and CV points that are complete in features 1-3.\n",
    "\n",
    "Etc. We will train as many models as the number of patterns in test/deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How to determine the patterns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 3)\n",
      "[False False False] 223\n",
      "[False False  True] 21\n",
      "[False  True False] 1\n",
      "[ True False False] 44\n",
      "[ True False  True] 2\n",
      "[ True  True False] 1\n"
     ]
    }
   ],
   "source": [
    "mask = df_test[['num__LotFrontage','num__MasVnrArea','num__GarageYrBlt']].isnull()\n",
    "unique_rows, counts = np.unique(mask, axis=0,return_counts=True)\n",
    "print(unique_rows.shape) # 6 patterns, we will train 6 models\n",
    "for i in range(len(counts)):\n",
    "    print(unique_rows[i],counts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import xgboost\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def xgb_model(X_train, Y_train, X_CV, y_CV, X_test, y_test, verbose=1):\n",
    "\n",
    "    # make into row vectors to avoid an obnoxious sklearn/xgb warning\n",
    "    Y_train = np.reshape(np.array(Y_train), (1, -1)).ravel()\n",
    "    y_CV = np.reshape(np.array(y_CV), (1, -1)).ravel()\n",
    "    y_test = np.reshape(np.array(y_test), (1, -1)).ravel()\n",
    "\n",
    "    XGB = xgboost.XGBRegressor(n_jobs=1)\n",
    "    \n",
    "    # find the best parameter set\n",
    "    param_grid = {\"learning_rate\": [0.03],\n",
    "                  \"n_estimators\": [10000],\n",
    "                  \"seed\": [0],\n",
    "                  #\"reg_alpha\": [0e0, 1e-2, 1e-1, 1e0, 1e1, 1e2],\n",
    "                  #\"reg_lambda\": [0e0, 1e-2, 1e-1, 1e0, 1e1, 1e2],\n",
    "                  \"missing\": [np.nan], \n",
    "                  #\"max_depth\": [1,3,10,30,100,],\n",
    "                  \"colsample_bytree\": [0.9],              \n",
    "                  \"subsample\": [0.66]}\n",
    "\n",
    "    pg = ParameterGrid(param_grid)\n",
    "\n",
    "    scores = np.zeros(len(pg))\n",
    "\n",
    "    for i in range(len(pg)):\n",
    "        if verbose >= 5:\n",
    "            print(\"Param set \" + str(i + 1) + \" / \" + str(len(pg)))\n",
    "        params = pg[i]\n",
    "        XGB.set_params(**params)\n",
    "        eval_set = [(X_CV, y_CV)]\n",
    "        XGB.fit(X_train, Y_train,\n",
    "                early_stopping_rounds=50, eval_set=eval_set, verbose=False)# with early stopping\n",
    "        y_CV_pred = XGB.predict(X_CV, ntree_limit=XGB.best_ntree_limit)\n",
    "        scores[i] = mean_squared_error(y_CV,y_CV_pred)\n",
    "\n",
    "    best_params = np.array(pg)[scores == np.max(scores)]\n",
    "    if verbose >= 4:\n",
    "        print('Test set max score and best parameters are:')\n",
    "        print(np.max(scores))\n",
    "        print(best_params)\n",
    "\n",
    "    # test the model on the test set with best parameter set\n",
    "    XGB.set_params(**best_params[0])\n",
    "    XGB.fit(X_train,Y_train,\n",
    "            early_stopping_rounds=50,eval_set=eval_set, verbose=False)\n",
    "    y_test_pred = XGB.predict(X_test, ntree_limit=XGB.best_ntree_limit)\n",
    "\n",
    "    if verbose >= 1:\n",
    "        print ('The MSE is:',mean_squared_error(y_test,y_test_pred))\n",
    "    if verbose >= 2:\n",
    "        print ('The predictions are:')\n",
    "        print (y_test_pred)\n",
    "    if verbose >= 3:\n",
    "        print(\"Feature importances:\")\n",
    "        print(XGB.feature_importances_)\n",
    "\n",
    "    return (mean_squared_error(y_test,y_test_pred), y_test_pred, XGB.feature_importances_)\n",
    "\n",
    "# Function: Reduced-feature XGB model\n",
    "# all the inputs need to be pandas DataFrame\n",
    "def reduced_feature_xgb(X_train, Y_train, X_CV, y_CV, X_test, y_test):\n",
    "    \n",
    "    # find all unique patterns of missing value in test set\n",
    "    mask = X_test.isnull()\n",
    "    unique_rows = np.array(np.unique(mask, axis=0))\n",
    "    all_y_test_pred = pd.DataFrame()\n",
    "    \n",
    "    print('there are', len(unique_rows), 'unique missing value patterns.')\n",
    "    \n",
    "    # divide test sets into subgroups according to the unique patterns\n",
    "    for i in range(len(unique_rows)):\n",
    "        print ('working on unique pattern', i)\n",
    "        ## generate X_test subset that matches the unique pattern i\n",
    "        sub_X_test = pd.DataFrame()\n",
    "        sub_y_test = pd.Series(dtype=float)\n",
    "        for j in range(len(mask)): # check each row in mask\n",
    "            row_mask = np.array(mask.iloc[j])\n",
    "            if np.array_equal(row_mask, unique_rows[i]): # if the pattern matches the ith unique pattern\n",
    "                sub_X_test = sub_X_test.append(X_test.iloc[j])# append the according X_test row j to the subset\n",
    "                sub_y_test = sub_y_test.append(y_test.iloc[[j]])# append the according y_test row j\n",
    "                                                \n",
    "        sub_X_test = sub_X_test[X_test.columns[~unique_rows[i]]]\n",
    "        \n",
    "        ## choose the according reduced features for subgroups\n",
    "        sub_X_train = pd.DataFrame()\n",
    "        sub_Y_train = pd.DataFrame()\n",
    "        sub_X_CV = pd.DataFrame()\n",
    "        sub_y_CV = pd.DataFrame()\n",
    "        # 1.cut the feature columns that have nans in the according sub_X_test\n",
    "        sub_X_train = X_train[X_train.columns[~unique_rows[i]]]\n",
    "        sub_X_CV = X_CV[X_CV.columns[~unique_rows[i]]]\n",
    "        # 2.cut the rows in the sub_X_train and sub_X_CV that have any nans\n",
    "        sub_X_train = sub_X_train.dropna()\n",
    "        sub_X_CV = sub_X_CV.dropna()   \n",
    "        # 3.cut the sub_Y_train and sub_y_CV accordingly\n",
    "        sub_Y_train = Y_train.iloc[sub_X_train.index]\n",
    "        sub_y_CV = y_CV.iloc[sub_X_CV.index]\n",
    "        \n",
    "        # run XGB\n",
    "        sub_y_test_pred = xgb_model(sub_X_train, sub_Y_train, sub_X_CV, \n",
    "                                       sub_y_CV, sub_X_test, sub_y_test, verbose=0)\n",
    "        sub_y_test_pred = pd.DataFrame(sub_y_test_pred[1],columns=['sub_y_test_pred'],\n",
    "                                          index=sub_y_test.index)\n",
    "        print('   RMSE:',np.sqrt(mean_squared_error(sub_y_test,sub_y_test_pred)))\n",
    "        # collect the test predictions\n",
    "        all_y_test_pred = all_y_test_pred.append(sub_y_test_pred)\n",
    "        \n",
    "    # rank the final y_test_pred according to original y_test index\n",
    "    all_y_test_pred = all_y_test_pred.sort_index()\n",
    "    y_test = y_test.sort_index()\n",
    "               \n",
    "    # get global RMSE\n",
    "    total_RMSE = np.sqrt(mean_squared_error(y_test,all_y_test_pred))\n",
    "    total_R2 =  r2_score(y_test,all_y_test_pred)\n",
    "    return total_RMSE, total_R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 6 unique missing value patterns.\n",
      "working on unique pattern 0\n",
      "   RMSE: 35277.53667892742\n",
      "working on unique pattern 1\n",
      "   RMSE: 11607.856743646213\n",
      "working on unique pattern 2\n",
      "   RMSE: 1134.5625\n",
      "working on unique pattern 3\n",
      "   RMSE: 18366.394043603428\n",
      "working on unique pattern 4\n",
      "   RMSE: 18521.340554971906\n",
      "working on unique pattern 5\n",
      "   RMSE: 65343.46875\n",
      "final RMSE: 32061.238747816282\n",
      "final R2: 0.8511518443924384\n"
     ]
    }
   ],
   "source": [
    "RMSE, R2 = reduced_feature_xgb(df_train, y_train, df_CV, y_CV, df_test, y_test)\n",
    "print('final RMSE:', RMSE)\n",
    "print('final R2:', R2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color='LIGHTGRAY'>Missing data</font>\n",
    "\n",
    "<font color='LIGHTGRAY'>By the end of this module, you will be able to</font>\n",
    "- <font color='LIGHTGRAY'>apply multivariate imputation</font>\n",
    "- <font color='LIGHTGRAY'>apply XGBoost to a dataset with missing values</font>\n",
    "- <font color='LIGHTGRAY'>apply the reduced-features model (also called the pattern submodel approach)</font>\n",
    "- **decide which approach is best for your dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Which approach is best for my data?\n",
    "- **XGB**: run $n$ XGB models with $n$ different seeds\n",
    "- **imputation**: prepare $n$ different imputations and run $n$ models on them\n",
    "- **reduced-features**: run $n$ reduced-features model with $n$ different seeds\n",
    "- rank the three methods based on how significantly different the corresponding mean scores are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mudcard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
