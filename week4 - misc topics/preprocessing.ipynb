{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mudcard\n",
    "- **can we work on time series data for the project?**\n",
    "    - yes, in fact you are encouraged to work with non-iid datasets\n",
    "    - that's how you learn the most in this class\n",
    "- **Can the autocorrelation plot help you pick an optimal lag period to use?**\n",
    "    - it might give you an idea but you really need to measure the generalization error as a function of the number of lag-shifted features to come up with an answer\n",
    "- **\"What is the proper lag-step and number of features for different scenarios?**\n",
    "    - as I said in class, there is no magic number that works for all cases\n",
    "    - measure how the generalization error changes as a function of the number of lag-shifted features \n",
    "    - that plot will give you an answer, but that answer will be specific to your project\n",
    "- **I'm still unsure about how to prevent info leak in time series, doesn't all future data contain info from the past?**\n",
    "    - the point is to not use future data to predict the past\n",
    "    - this might seem obvious but it is one of those mistakes that are easy to make because your code will not produce an error message\n",
    "- **Why does introducing lag features make the feature matrix iid?**\n",
    "    - it doesn't\n",
    "    - maybe you have an earlier version of the lecture notes, I made some changes on tuesday morning\n",
    "    - check out the current version\n",
    "- **I am still very unclear about what the \"lag\" is or how the \"shifting\" results in iid data.**\n",
    "    - shifting does not result in iid data\n",
    "    - maybe you also have access to the earlier version of the lecture notes\n",
    "- **Why the autocorrelation plot decreases so smoothly in this pattern?**\n",
    "- **can you explain the autocorrelation figure? why with the increasing lag, the correlation coefficienct convergent to 0**\n",
    "    - check the code in the previous lecture\n",
    "    - the autocorrelation coefficient has two terms: the Pearson correlation coefficient multiplied by the fraction of the dataset used\n",
    "    - that second term approaches 0 as the leg increases\n",
    "    - it is 1 for 0 leg, and 0 for the maximum lag you can calculate based on your dataset\n",
    "- **Why do we have to make time lags for the weather data to predict future outcomes? Why cant we just take every other day and put in training set, and the remaining go in testing - or something similar with a not 50/50 split?**\n",
    "    - because one time series measurement does not give you a feature matrix and a target variable\n",
    "    - you need to transform it\n",
    "    - you can't do a simple train_test_split because time series datasets are not iid\n",
    "- **For time series, is the number of features inversely correlated with the number of observations for each one?**\n",
    "    - Linearly correlated\n",
    "    - if you have more observations, the maxmimum number of lag-shifted features you can create goes up\n",
    "- **Why do we use autocorrelation and autoregression?**\n",
    "    - to study and work with time series data\n",
    "- **If your target variable is very far off in the future. How do you test for the accuracy of your prediction when you only have data form the past?**\n",
    "    - you usually predict near future stuff\n",
    "    - predictive power far in the future is usually pretty bad otherwise ML would be much widely used\n",
    "- **For the wrist-band hospital model, would more of the same kind of data allow us to build a better model?**\n",
    "    - that's the hope, yes\n",
    "    - it is not guaranteed though\n",
    "    - the conclusion of the paper was that there might be something promising here but we will need more data to verify\n",
    "- **What is the best way to balance group splitting and stratified splitting? For example, if a class shows up in only one group, we may train models that do not ever see that class in training, but we also dont want to split the group. Is there a compromise between these two factors?**\n",
    "    - good question!\n",
    "    - I think it depends on how many grops you have\n",
    "    - if you have many groups, this is not a problem because you will have multiple groups in each set\n",
    "    - if you only have a limited number of groups, it might happen that your validation and/or test set will only contain that group and it will be difficult to assess the generalization error\n",
    "- **So when making features for time-series data, these are all just lags?**\n",
    "    - that's the basic idea, yes\n",
    "    - you can add moving averages instead of the feature values\n",
    "    - but generally lag shifting is what you do with time series data in ML\n",
    "- **Do we just have one column of data (but being shifted differently into different columns) for autoregression?**\n",
    "    - yep\n",
    "    - you might have multiple time series observations, e.g., you might want to predict the S&P 500 price using the Tesla, Meta, and Amazon prices. You would need to shift all three price feeds to generate features\n",
    "- **What does the term \"data-leakage\" mean exactly?**\n",
    "    - it means that you use information during model development that will not be available to you when the model is deployed\n",
    "    - as a result, your model gives a great test score (generalization error), but it will perform poorly when it is deployed\n",
    "- **When should you use auto correlation? It intuitively feel like past values might negatively influence the prediction. Unless something is very predictable like temperature or weekly purchases, I feel like taking in account of past prices could be bad. It feels weird like cofounding variables.**\n",
    "    - using the past data to predict the future is perfectly fine\n",
    "    - that's mostly what people use to predict any sort of price movements\n",
    "    - what other info would you use as features?\n",
    "- **For predicting time series data, can we use an ARIMA model?**\n",
    "    - no, stick to autoregressive features and apply the ML models we will cover in a few weeks during class\n",
    "- **What would be a more intuitive way to understand these segmented bar charts for GroupShuffleSplit, GroupKFold, etc.? Currently not sure how the colored bars link to the concept**\n",
    "    - read the manuals on the sklearn website\n",
    "    - they provide longer explanations and more examples\n",
    "- **Is it worthwhile to see how a target variable correlates to a lag of different features you have?**\n",
    "    - if those other features are time series of other observations, yes\n",
    "    - you only lag-shift time series observations\n",
    "- **\"I only have a vague understanding on the underlying purpose of using lags on time-series data, could you give some further explanation?**\n",
    "    - check the sklearn website for more explanations and examples [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html)\n",
    "- **For the varieties of auto-regression that would show up in technical interviews, how deep should we know about it? Should we know how to actually imply them or to be able to explain the mechanism is fine?**\n",
    "    - the more you know, the better\n",
    "- **I am generally confused about the topic of autoregression and why one reorients features. Is it solely due based on time, or is there other considerations?**\n",
    "    - yes, you would apply autoregression on time series data only\n",
    "- **It would be helpful to fully walk through a time series analysis. What model would we use to predict the target variable? How do we measure the accuracy of our model as we try to predict more distant target variables?**\n",
    "    - once the features are generated via autoregression, the rest of the steps are the same as for an iid or any other dataset\n",
    "    - we will cover all of these topics in the coming weeks\n",
    "- **I think I became more confused with the concept of K-fold and n_splits. Is there a really helpful crash course video that you would recommend in addition to watching the lecture over again?**\n",
    "    - check out the sklearn website for more examples and more explanation\n",
    "- **Could you explain more about the GroupKFold?**\n",
    "    - I won't have time but check out the sklearn website for more info\n",
    "- **And also for the part of time series, will we learn more about it? (like ARMA, SARIMA in this class)**\n",
    "    - no, we won't cover more in this class, I just gave you the big picture overview and idea\n",
    "    - but i do recommend you read more about those techniques\n",
    "- **I am unclear how you calculate the confidence interval in the autocorrelation plot.**\n",
    "    - pandas calculates it for you\n",
    "    - check the pandas manual for an explanation\n",
    "- **If there are non-time series features combined with time-series features, how do we train the model? Do we train the time-series \"lag features\" first, then we modify the model based on non-time series features? Training all the features at the same time does not seem feasible to me.**\n",
    "    - it is feasable, you simply need to merge the time series features with the non-time series features\n",
    "- **What are the advantages of group-based split instead of a Group Shuffle Split? When exactly we should choose one instead of the other?**\n",
    "- **Would you be able to clarify we should use¬† GroupKFold vs GroupShuffleFold?**\n",
    "    - both are fine\n",
    "    - I'm giving you multiple tools to solve the same problem\n",
    "    - I have a small personal preference for GroupKFold but that's subjective\n",
    "- **Im still unclear what the main advantage/use of doing autocorrelation and lag would be? Is it exclusive for timeseries data?**\n",
    "    - yes it is\n",
    "- **Will we have much time series data?**\n",
    "    - you can use a time series prediction for your project\n",
    "    - and it is almost certain that you will come across time series data in your career at some point\n",
    "- **What do you do with uncorrelated time series data?**\n",
    "    - good question\n",
    "    - I have not seen uncorrelated time series data\n",
    "    - if the autocorrelation plot is flat with a correlation coefficient 0, it means you should not create autoregressive features, you need to collect some other features for prediction\n",
    "- **What does it mean for data to be iid, or not iid, and why do other correlations not count as violating iid data?**\n",
    "    - iid vs non-iid is about how the data is generated, it is not about correlations between the features and the target variable\n",
    "- **Very muddy about the time series scenarios and why we even need to use machine learning for those types of problems. You're just calculating everything out no?**\n",
    "    - what do you mean by 'calculating everything out'?\n",
    "- **What is the purpose of the Pearson coefficient ?**\n",
    "    - check out [here](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient)\n",
    "- **Sorry that this is a basic question: I'm a bit confused -- in a groupKfolds split, does each group get put in a fold by itself? How does that differentiate conceptually from GroupShuffleSplit?**\n",
    "    - yes, in groupkfold, each group gets put in a fold by itself, so each group appears only once in the validation set\n",
    "    - in groupshufflesplit, the groups in the validation are determined randomly, so the same group can be put in the validation set multiple times\n",
    "- **I want to more practice during the class!**\n",
    "    - me too but there is no time\n",
    "    - you need to practice outside of class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Data preprocessing</center>\n",
    "### By the end of this lecture, you will be able to\n",
    "- apply one-hot encoding or ordinal encoding to categorical variables\n",
    "- apply scaling and normalization to continuous variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The supervised ML pipeline\n",
    "The goal: Use the training data (X and y) to develop a <font color='red'>model</font> which can <font color='red'>accurately</font> predict the target variable (y_new') for previously unseen data (X_new).\n",
    "\n",
    "**1. Exploratory Data Analysis (EDA)**: you need to understand your data and verify that it doesn't contain errors\n",
    "   - do as much EDA as you can!\n",
    "    \n",
    "**2. Split the data into different sets**: most often the sets are train, validation, and test (or holdout)\n",
    "   - practitioners often make errors in this step!\n",
    "   - you can split the data randomly, based on groups, based on time, or any other non-standard way if necessary to answer your ML question\n",
    "\n",
    "<span style=\"background-color: #FFFF00\">**3. Preprocess the data**: ML models only work if X and Y are numbers! Some ML models additionally require each feature to have 0 mean and 1 standard deviation (standardized features)</span>\n",
    "   - often the original features you get contain strings (for example a gender feature would contain 'male', 'female', 'non-binary', 'unknown') which needs to transformed into numbers\n",
    "   - often the features are not standardized (e.g., age is between 0 and 100) but it needs to be standardized\n",
    "    \n",
    "**4. Choose an evaluation metric**: depends on the priorities of the stakeholders\n",
    "   - often requires quite a bit of thinking and ethical considerations\n",
    "     \n",
    "**5. Choose one or more ML techniques**: it is highly recommended that you try multiple models\n",
    "   - start with simple models like linear or logistic regression\n",
    "   - try also more complex models like nearest neighbors, support vector machines, random forest, etc.\n",
    "    \n",
    "**6. Tune the hyperparameters of your ML models (aka cross-validation)**\n",
    "   - ML techniques have hyperparameters that you need to optimize to achieve best performance\n",
    "   - for each ML model, decide which parameters to tune and what values to try\n",
    "   - loop through each parameter combination\n",
    "       - train one model for each parameter combination\n",
    "       - evaluate how well the model performs on the validation set\n",
    "   - take the parameter combo that gives the best validation score\n",
    "   - evaluate that model on the test set to report how well the model is expected to perform on previously unseen data\n",
    "    \n",
    "**7. Interpret your model**: black boxes are often not useful\n",
    "   - check if your model uses features that make sense (excellent tool for debugging)\n",
    "   - often model predictions are not enough, you need to be able to explain how the model arrived to a particular prediction (e.g., in health care)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problem description, why preprocessing is necessary\n",
    "\n",
    "Data format suitable for ML: 2D numerical values.\n",
    "\n",
    "| X|feature_1|feature_2|...|feature_j|...|feature_m|<font color='red'>y</font>|\n",
    "|-|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|__data_point_1__|x_11|x_12|...|x_1j|...|x_1m|__<font color='red'>y_1</font>__|\n",
    "|__data_point_2__|x_21|x_22|...|x_2j|...|x_2m|__<font color='red'>y_2</font>__|\n",
    "|__...__|...|...|...|...|...|...|__<font color='red'>...</font>__|\n",
    "|__data_point_i__|x_i1|x_i2|...|x_ij|...|x_im|__<font color='red'>y_i</font>__|\n",
    "|__...__|...|...|...|...|...|...|__<font color='red'>...</font>__|\n",
    "|__data_point_n__|x_n1|x_n2|...|x_nj|...|x_nm|__<font color='red'>y_n</font>__|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Data almost never comes in a format that's directly usable in ML.\n",
    "- let's check the adult data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set\n",
      "       age   workclass  fnlwgt      education  education-num  \\\n",
      "25823   31     Private   87418      Assoc-voc             11   \n",
      "10274   41     Private  121718   Some-college             10   \n",
      "27652   61     Private   79827        HS-grad              9   \n",
      "13941   33   State-gov  156015      Bachelors             13   \n",
      "31384   38     Private  167882   Some-college             10   \n",
      "\n",
      "            marital-status        occupation     relationship    race  \\\n",
      "25823   Married-civ-spouse   Exec-managerial          Husband   White   \n",
      "10274   Married-civ-spouse      Craft-repair          Husband   White   \n",
      "27652   Married-civ-spouse   Exec-managerial          Husband   White   \n",
      "13941   Married-civ-spouse   Exec-managerial          Husband   White   \n",
      "31384              Widowed     Other-service   Other-relative   Black   \n",
      "\n",
      "           sex  capital-gain  capital-loss  hours-per-week  native-country  \n",
      "25823     Male             0             0              40   United-States  \n",
      "10274     Male             0             0              40           Italy  \n",
      "27652     Male             0             0              50   United-States  \n",
      "13941     Male             0             0              40   United-States  \n",
      "31384   Female             0             0              45           Haiti  \n",
      "25823     <=50K\n",
      "10274     <=50K\n",
      "27652     <=50K\n",
      "13941      >50K\n",
      "31384     <=50K\n",
      "Name: gross-income, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "df = pd.read_csv('data/adult_data.csv')\n",
    "\n",
    "# let's separate the feature matrix X, and target variable y\n",
    "y = df['gross-income'] # remember, we want to predict who earns more than 50k or less than 50k\n",
    "X = df.loc[:, df.columns != 'gross-income'] # all other columns are features\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "# first split to separate out the training set\n",
    "X_train, X_other, y_train, y_other = train_test_split(X,y,train_size = 0.6,random_state=random_state)\n",
    "\n",
    "# second split to separate out the validation and test sets\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_other,y_other,train_size = 0.5,random_state=random_state)\n",
    "\n",
    "print('training set')\n",
    "print(X_train.head()) # lots of strings!\n",
    "print(y_train.head()) # even our labels are strings and not numbers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### scikit-learn transformers to the rescue!\n",
    "\n",
    "Preprocessing is done with various transformers. All transformes have three methods:\n",
    "- **fit** method: estimates parameters necessary to do the transformation,\n",
    "- **transform** method: transforms the data based on the estimated parameters,\n",
    "- **fit_transform** method: both steps are performed at once, this can be faster than doing the steps separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Transformers we cover today\n",
    "- **OneHotEncoder** - converts categorical features into dummy arrays\n",
    "- **OrdinalEncoder** - converts categorical features into an integer array\n",
    "- **MinMaxScaler** - scales continuous variables to be between 0 and 1\n",
    "- **StandardScaler** - standardizes continuous features by removing the mean and scaling to unit variance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <font color='LIGHTGRAY'>By the end of this lecture, you will be able to</font>\n",
    "- **apply one-hot encoding or ordinal encoding to categorical variables**\n",
    "- <font color='LIGHTGRAY'>apply scaling and normalization to continuous variables</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ordered categorical data: OrdinalEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- use it on categorical features if the categories can be ranked or ordered\n",
    "    - educational level in the adult dataset\n",
    "    - reaction to medication is described by words like 'severe', 'no response', 'excellent'\n",
    "    - any time you know that the categories can be clearly ranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class OrdinalEncoder in module sklearn.preprocessing._encoders:\n",
      "\n",
      "class OrdinalEncoder(sklearn.base._OneToOneFeatureMixin, _BaseEncoder)\n",
      " |  OrdinalEncoder(*, categories='auto', dtype=<class 'numpy.float64'>, handle_unknown='error', unknown_value=None, encoded_missing_value=nan)\n",
      " |  \n",
      " |  Encode categorical features as an integer array.\n",
      " |  \n",
      " |  The input to this transformer should be an array-like of integers or\n",
      " |  strings, denoting the values taken on by categorical (discrete) features.\n",
      " |  The features are converted to ordinal integers. This results in\n",
      " |  a single column of integers (0 to n_categories - 1) per feature.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <preprocessing_categorical_features>`.\n",
      " |  \n",
      " |  .. versionadded:: 0.20\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  categories : 'auto' or a list of array-like, default='auto'\n",
      " |      Categories (unique values) per feature:\n",
      " |  \n",
      " |      - 'auto' : Determine categories automatically from the training data.\n",
      " |      - list : ``categories[i]`` holds the categories expected in the ith\n",
      " |        column. The passed categories should not mix strings and numeric\n",
      " |        values, and should be sorted in case of numeric values.\n",
      " |  \n",
      " |      The used categories can be found in the ``categories_`` attribute.\n",
      " |  \n",
      " |  dtype : number type, default np.float64\n",
      " |      Desired dtype of output.\n",
      " |  \n",
      " |  handle_unknown : {'error', 'use_encoded_value'}, default='error'\n",
      " |      When set to 'error' an error will be raised in case an unknown\n",
      " |      categorical feature is present during transform. When set to\n",
      " |      'use_encoded_value', the encoded value of unknown categories will be\n",
      " |      set to the value given for the parameter `unknown_value`. In\n",
      " |      :meth:`inverse_transform`, an unknown category will be denoted as None.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  unknown_value : int or np.nan, default=None\n",
      " |      When the parameter handle_unknown is set to 'use_encoded_value', this\n",
      " |      parameter is required and will set the encoded value of unknown\n",
      " |      categories. It has to be distinct from the values used to encode any of\n",
      " |      the categories in `fit`. If set to np.nan, the `dtype` parameter must\n",
      " |      be a float dtype.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  encoded_missing_value : int or np.nan, default=np.nan\n",
      " |      Encoded value of missing categories. If set to `np.nan`, then the `dtype`\n",
      " |      parameter must be a float dtype.\n",
      " |  \n",
      " |      .. versionadded:: 1.1\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  categories_ : list of arrays\n",
      " |      The categories of each feature determined during ``fit`` (in order of\n",
      " |      the features in X and corresponding with the output of ``transform``).\n",
      " |      This does not include categories that weren't seen during ``fit``.\n",
      " |  \n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`.\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      " |      has feature names that are all strings.\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  OneHotEncoder : Performs a one-hot encoding of categorical features.\n",
      " |  LabelEncoder : Encodes target labels with values between 0 and\n",
      " |      ``n_classes-1``.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  Given a dataset with two features, we let the encoder find the unique\n",
      " |  values per feature and transform the data to an ordinal encoding.\n",
      " |  \n",
      " |  >>> from sklearn.preprocessing import OrdinalEncoder\n",
      " |  >>> enc = OrdinalEncoder()\n",
      " |  >>> X = [['Male', 1], ['Female', 3], ['Female', 2]]\n",
      " |  >>> enc.fit(X)\n",
      " |  OrdinalEncoder()\n",
      " |  >>> enc.categories_\n",
      " |  [array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)]\n",
      " |  >>> enc.transform([['Female', 3], ['Male', 1]])\n",
      " |  array([[0., 2.],\n",
      " |         [1., 0.]])\n",
      " |  \n",
      " |  >>> enc.inverse_transform([[1, 0], [0, 1]])\n",
      " |  array([['Male', 1],\n",
      " |         ['Female', 2]], dtype=object)\n",
      " |  \n",
      " |  By default, :class:`OrdinalEncoder` is lenient towards missing values by\n",
      " |  propagating them.\n",
      " |  \n",
      " |  >>> import numpy as np\n",
      " |  >>> X = [['Male', 1], ['Female', 3], ['Female', np.nan]]\n",
      " |  >>> enc.fit_transform(X)\n",
      " |  array([[ 1.,  0.],\n",
      " |         [ 0.,  1.],\n",
      " |         [ 0., nan]])\n",
      " |  \n",
      " |  You can use the parameter `encoded_missing_value` to encode missing values.\n",
      " |  \n",
      " |  >>> enc.set_params(encoded_missing_value=-1).fit_transform(X)\n",
      " |  array([[ 1.,  0.],\n",
      " |         [ 0.,  1.],\n",
      " |         [ 0., -1.]])\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      OrdinalEncoder\n",
      " |      sklearn.base._OneToOneFeatureMixin\n",
      " |      _BaseEncoder\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, categories='auto', dtype=<class 'numpy.float64'>, handle_unknown='error', unknown_value=None, encoded_missing_value=nan)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None)\n",
      " |      Fit the OrdinalEncoder to X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          The data to determine the categories of each feature.\n",
      " |      \n",
      " |      y : None\n",
      " |          Ignored. This parameter exists only for compatibility with\n",
      " |          :class:`~sklearn.pipeline.Pipeline`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Fitted encoder.\n",
      " |  \n",
      " |  inverse_transform(self, X)\n",
      " |      Convert the data back to the original representation.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_encoded_features)\n",
      " |          The transformed data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_tr : ndarray of shape (n_samples, n_features)\n",
      " |          Inverse transformed array.\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Transform X to ordinal codes.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          The data to encode.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_out : ndarray of shape (n_samples, n_features)\n",
      " |          Transformed input.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base._OneToOneFeatureMixin:\n",
      " |  \n",
      " |  get_feature_names_out(self, input_features=None)\n",
      " |      Get output feature names for transformation.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      input_features : array-like of str or None, default=None\n",
      " |          Input features.\n",
      " |      \n",
      " |          - If `input_features` is `None`, then `feature_names_in_` is\n",
      " |            used as feature names in. If `feature_names_in_` is not defined,\n",
      " |            then the following input feature names are generated:\n",
      " |            `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n",
      " |          - If `input_features` is an array-like, then `input_features` must\n",
      " |            match `feature_names_in_` if `feature_names_in_` is defined.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_names_out : ndarray of str objects\n",
      " |          Same as input features.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base._OneToOneFeatureMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  fit_transform(self, X, y=None, **fit_params)\n",
      " |      Fit to data, then transform it.\n",
      " |      \n",
      " |      Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      " |      and returns a transformed version of `X`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Input samples.\n",
      " |      \n",
      " |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      " |          Target values (None for unsupervised transformations).\n",
      " |      \n",
      " |      **fit_params : dict\n",
      " |          Additional fit parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      " |          Transformed array.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "help(OrdinalEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['HS-grad', 'College', 'Bachelors', 'Masters', 'Doctorate'],\n",
      "      dtype=object)]\n",
      "[[2.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [0.]\n",
      " [3.]]\n",
      "[[0.]\n",
      " [3.]\n",
      " [3.]\n",
      " [1.]\n",
      " [2.]]\n"
     ]
    }
   ],
   "source": [
    "# toy example\n",
    "import pandas as pd\n",
    "\n",
    "train_edu = {'educational level':['Bachelors','Masters','Bachelors','Doctorate','HS-grad','Masters']} \n",
    "test_edu = {'educational level':['HS-grad','Masters','Masters','College','Bachelors']}\n",
    "\n",
    "Xtoy_train = pd.DataFrame(train_edu)\n",
    "Xtoy_test = pd.DataFrame(test_edu)\n",
    "\n",
    "# initialize the encoder\n",
    "cats = [['HS-grad','College','Bachelors','Masters','Doctorate']]\n",
    "\n",
    "enc = OrdinalEncoder(categories = cats) # The ordered list of \n",
    "# categories need to be provided. By default, the categories are alphabetically ordered!\n",
    "\n",
    "# fit the training data\n",
    "enc.fit(Xtoy_train)\n",
    "# print the categories - not really important because we manually gave the ordered list of categories\n",
    "print(enc.categories_)\n",
    "# transform X_train. We could have used enc.fit_transform(X_train) to combine fit and transform\n",
    "X_train_oe = enc.transform(Xtoy_train)\n",
    "print(X_train_oe)\n",
    "# transform X_test\n",
    "X_test_oe = enc.transform(Xtoy_test) # OrdinalEncoder always throws an error message if \n",
    "                                  # it encounters an unknown category in test\n",
    "print(X_test_oe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformed train features:\n",
      "[[10.]\n",
      " [ 9.]\n",
      " [ 8.]\n",
      " ...\n",
      " [ 6.]\n",
      " [ 8.]\n",
      " [12.]]\n",
      "transformed validation features:\n",
      "[[14.]\n",
      " [13.]\n",
      " [ 9.]\n",
      " ...\n",
      " [12.]\n",
      " [ 8.]\n",
      " [ 8.]]\n",
      "transformed test features:\n",
      "[[12.]\n",
      " [ 9.]\n",
      " [12.]\n",
      " ...\n",
      " [ 9.]\n",
      " [ 9.]\n",
      " [11.]]\n"
     ]
    }
   ],
   "source": [
    "# apply OE to the adult dataset\n",
    "# initialize the encoder\n",
    "ordinal_ftrs = ['education'] # if you have more than one ordinal feature, add the feature names here\n",
    "ordinal_cats = [[' Preschool',' 1st-4th',' 5th-6th',' 7th-8th',' 9th',' 10th',' 11th',' 12th',' HS-grad',\\\n",
    "                ' Some-college',' Assoc-voc',' Assoc-acdm',' Bachelors',' Masters',' Prof-school',' Doctorate']]\n",
    "# ordinal_cats must contain one list per ordinal feature! each list contains the ordered list of categories \n",
    "# of the corresponding feature\n",
    "\n",
    "enc = OrdinalEncoder(categories = ordinal_cats)   # By default, the categories are alphabetically ordered\n",
    "                                                    # which is NOT what you want usually.\n",
    "\n",
    "# fit the training data\n",
    "enc.fit(X_train[ordinal_ftrs])  # the encoder expects a 2D array, that's why the column name is in a list\n",
    "\n",
    "# transform X_train. We could use enc.fit_transform(X_train) to combine fit and transform\n",
    "ordinal_train = enc.transform(X_train[ordinal_ftrs])\n",
    "print('transformed train features:')\n",
    "print(ordinal_train)\n",
    "# transform X_val\n",
    "ordinal_val = enc.transform(X_val[ordinal_ftrs])\n",
    "print('transformed validation features:')\n",
    "print(ordinal_val)\n",
    "# transform X_test\n",
    "ordinal_test = enc.transform(X_test[ordinal_ftrs])\n",
    "print('transformed test features:')\n",
    "print(ordinal_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unordered categorical data: one-hot encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- some categories cannot be ordered. e.g., workclass, relationship status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class OneHotEncoder in module sklearn.preprocessing._encoders:\n",
      "\n",
      "class OneHotEncoder(_BaseEncoder)\n",
      " |  OneHotEncoder(*, categories='auto', drop=None, sparse=True, dtype=<class 'numpy.float64'>, handle_unknown='error', min_frequency=None, max_categories=None)\n",
      " |  \n",
      " |  Encode categorical features as a one-hot numeric array.\n",
      " |  \n",
      " |  The input to this transformer should be an array-like of integers or\n",
      " |  strings, denoting the values taken on by categorical (discrete) features.\n",
      " |  The features are encoded using a one-hot (aka 'one-of-K' or 'dummy')\n",
      " |  encoding scheme. This creates a binary column for each category and\n",
      " |  returns a sparse matrix or dense array (depending on the ``sparse``\n",
      " |  parameter)\n",
      " |  \n",
      " |  By default, the encoder derives the categories based on the unique values\n",
      " |  in each feature. Alternatively, you can also specify the `categories`\n",
      " |  manually.\n",
      " |  \n",
      " |  This encoding is needed for feeding categorical data to many scikit-learn\n",
      " |  estimators, notably linear models and SVMs with the standard kernels.\n",
      " |  \n",
      " |  Note: a one-hot encoding of y labels should use a LabelBinarizer\n",
      " |  instead.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <preprocessing_categorical_features>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  categories : 'auto' or a list of array-like, default='auto'\n",
      " |      Categories (unique values) per feature:\n",
      " |  \n",
      " |      - 'auto' : Determine categories automatically from the training data.\n",
      " |      - list : ``categories[i]`` holds the categories expected in the ith\n",
      " |        column. The passed categories should not mix strings and numeric\n",
      " |        values within a single feature, and should be sorted in case of\n",
      " |        numeric values.\n",
      " |  \n",
      " |      The used categories can be found in the ``categories_`` attribute.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |  \n",
      " |  drop : {'first', 'if_binary'} or an array-like of shape (n_features,),             default=None\n",
      " |      Specifies a methodology to use to drop one of the categories per\n",
      " |      feature. This is useful in situations where perfectly collinear\n",
      " |      features cause problems, such as when feeding the resulting data\n",
      " |      into an unregularized linear regression model.\n",
      " |  \n",
      " |      However, dropping one category breaks the symmetry of the original\n",
      " |      representation and can therefore induce a bias in downstream models,\n",
      " |      for instance for penalized linear classification or regression models.\n",
      " |  \n",
      " |      - None : retain all features (the default).\n",
      " |      - 'first' : drop the first category in each feature. If only one\n",
      " |        category is present, the feature will be dropped entirely.\n",
      " |      - 'if_binary' : drop the first category in each feature with two\n",
      " |        categories. Features with 1 or more than 2 categories are\n",
      " |        left intact.\n",
      " |      - array : ``drop[i]`` is the category in feature ``X[:, i]`` that\n",
      " |        should be dropped.\n",
      " |  \n",
      " |      .. versionadded:: 0.21\n",
      " |         The parameter `drop` was added in 0.21.\n",
      " |  \n",
      " |      .. versionchanged:: 0.23\n",
      " |         The option `drop='if_binary'` was added in 0.23.\n",
      " |  \n",
      " |      .. versionchanged:: 1.1\n",
      " |          Support for dropping infrequent categories.\n",
      " |  \n",
      " |  sparse : bool, default=True\n",
      " |      Will return sparse matrix if set True else will return an array.\n",
      " |  \n",
      " |  dtype : number type, default=float\n",
      " |      Desired dtype of output.\n",
      " |  \n",
      " |  handle_unknown : {'error', 'ignore', 'infrequent_if_exist'},                      default='error'\n",
      " |      Specifies the way unknown categories are handled during :meth:`transform`.\n",
      " |  \n",
      " |      - 'error' : Raise an error if an unknown category is present during transform.\n",
      " |      - 'ignore' : When an unknown category is encountered during\n",
      " |        transform, the resulting one-hot encoded columns for this feature\n",
      " |        will be all zeros. In the inverse transform, an unknown category\n",
      " |        will be denoted as None.\n",
      " |      - 'infrequent_if_exist' : When an unknown category is encountered\n",
      " |        during transform, the resulting one-hot encoded columns for this\n",
      " |        feature will map to the infrequent category if it exists. The\n",
      " |        infrequent category will be mapped to the last position in the\n",
      " |        encoding. During inverse transform, an unknown category will be\n",
      " |        mapped to the category denoted `'infrequent'` if it exists. If the\n",
      " |        `'infrequent'` category does not exist, then :meth:`transform` and\n",
      " |        :meth:`inverse_transform` will handle an unknown category as with\n",
      " |        `handle_unknown='ignore'`. Infrequent categories exist based on\n",
      " |        `min_frequency` and `max_categories`. Read more in the\n",
      " |        :ref:`User Guide <one_hot_encoder_infrequent_categories>`.\n",
      " |  \n",
      " |      .. versionchanged:: 1.1\n",
      " |          `'infrequent_if_exist'` was added to automatically handle unknown\n",
      " |          categories and infrequent categories.\n",
      " |  \n",
      " |  min_frequency : int or float, default=None\n",
      " |      Specifies the minimum frequency below which a category will be\n",
      " |      considered infrequent.\n",
      " |  \n",
      " |      - If `int`, categories with a smaller cardinality will be considered\n",
      " |        infrequent.\n",
      " |  \n",
      " |      - If `float`, categories with a smaller cardinality than\n",
      " |        `min_frequency * n_samples`  will be considered infrequent.\n",
      " |  \n",
      " |      .. versionadded:: 1.1\n",
      " |          Read more in the :ref:`User Guide <one_hot_encoder_infrequent_categories>`.\n",
      " |  \n",
      " |  max_categories : int, default=None\n",
      " |      Specifies an upper limit to the number of output features for each input\n",
      " |      feature when considering infrequent categories. If there are infrequent\n",
      " |      categories, `max_categories` includes the category representing the\n",
      " |      infrequent categories along with the frequent categories. If `None`,\n",
      " |      there is no limit to the number of output features.\n",
      " |  \n",
      " |      .. versionadded:: 1.1\n",
      " |          Read more in the :ref:`User Guide <one_hot_encoder_infrequent_categories>`.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  categories_ : list of arrays\n",
      " |      The categories of each feature determined during fitting\n",
      " |      (in order of the features in X and corresponding with the output\n",
      " |      of ``transform``). This includes the category specified in ``drop``\n",
      " |      (if any).\n",
      " |  \n",
      " |  drop_idx_ : array of shape (n_features,)\n",
      " |      - ``drop_idx_[i]`` is the index in ``categories_[i]`` of the category\n",
      " |        to be dropped for each feature.\n",
      " |      - ``drop_idx_[i] = None`` if no category is to be dropped from the\n",
      " |        feature with index ``i``, e.g. when `drop='if_binary'` and the\n",
      " |        feature isn't binary.\n",
      " |      - ``drop_idx_ = None`` if all the transformed features will be\n",
      " |        retained.\n",
      " |  \n",
      " |      If infrequent categories are enabled by setting `min_frequency` or\n",
      " |      `max_categories` to a non-default value and `drop_idx[i]` corresponds\n",
      " |      to a infrequent category, then the entire infrequent category is\n",
      " |      dropped.\n",
      " |  \n",
      " |      .. versionchanged:: 0.23\n",
      " |         Added the possibility to contain `None` values.\n",
      " |  \n",
      " |  infrequent_categories_ : list of ndarray\n",
      " |      Defined only if infrequent categories are enabled by setting\n",
      " |      `min_frequency` or `max_categories` to a non-default value.\n",
      " |      `infrequent_categories_[i]` are the infrequent categories for feature\n",
      " |      `i`. If the feature `i` has no infrequent categories\n",
      " |      `infrequent_categories_[i]` is None.\n",
      " |  \n",
      " |      .. versionadded:: 1.1\n",
      " |  \n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`.\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      " |      has feature names that are all strings.\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  OrdinalEncoder : Performs an ordinal (integer)\n",
      " |    encoding of the categorical features.\n",
      " |  sklearn.feature_extraction.DictVectorizer : Performs a one-hot encoding of\n",
      " |    dictionary items (also handles string-valued features).\n",
      " |  sklearn.feature_extraction.FeatureHasher : Performs an approximate one-hot\n",
      " |    encoding of dictionary items or strings.\n",
      " |  LabelBinarizer : Binarizes labels in a one-vs-all\n",
      " |    fashion.\n",
      " |  MultiLabelBinarizer : Transforms between iterable of\n",
      " |    iterables and a multilabel format, e.g. a (samples x classes) binary\n",
      " |    matrix indicating the presence of a class label.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  Given a dataset with two features, we let the encoder find the unique\n",
      " |  values per feature and transform the data to a binary one-hot encoding.\n",
      " |  \n",
      " |  >>> from sklearn.preprocessing import OneHotEncoder\n",
      " |  \n",
      " |  One can discard categories not seen during `fit`:\n",
      " |  \n",
      " |  >>> enc = OneHotEncoder(handle_unknown='ignore')\n",
      " |  >>> X = [['Male', 1], ['Female', 3], ['Female', 2]]\n",
      " |  >>> enc.fit(X)\n",
      " |  OneHotEncoder(handle_unknown='ignore')\n",
      " |  >>> enc.categories_\n",
      " |  [array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)]\n",
      " |  >>> enc.transform([['Female', 1], ['Male', 4]]).toarray()\n",
      " |  array([[1., 0., 1., 0., 0.],\n",
      " |         [0., 1., 0., 0., 0.]])\n",
      " |  >>> enc.inverse_transform([[0, 1, 1, 0, 0], [0, 0, 0, 1, 0]])\n",
      " |  array([['Male', 1],\n",
      " |         [None, 2]], dtype=object)\n",
      " |  >>> enc.get_feature_names_out(['gender', 'group'])\n",
      " |  array(['gender_Female', 'gender_Male', 'group_1', 'group_2', 'group_3'], ...)\n",
      " |  \n",
      " |  One can always drop the first column for each feature:\n",
      " |  \n",
      " |  >>> drop_enc = OneHotEncoder(drop='first').fit(X)\n",
      " |  >>> drop_enc.categories_\n",
      " |  [array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)]\n",
      " |  >>> drop_enc.transform([['Female', 1], ['Male', 2]]).toarray()\n",
      " |  array([[0., 0., 0.],\n",
      " |         [1., 1., 0.]])\n",
      " |  \n",
      " |  Or drop a column for feature only having 2 categories:\n",
      " |  \n",
      " |  >>> drop_binary_enc = OneHotEncoder(drop='if_binary').fit(X)\n",
      " |  >>> drop_binary_enc.transform([['Female', 1], ['Male', 2]]).toarray()\n",
      " |  array([[0., 1., 0., 0.],\n",
      " |         [1., 0., 1., 0.]])\n",
      " |  \n",
      " |  Infrequent categories are enabled by setting `max_categories` or `min_frequency`.\n",
      " |  \n",
      " |  >>> import numpy as np\n",
      " |  >>> X = np.array([[\"a\"] * 5 + [\"b\"] * 20 + [\"c\"] * 10 + [\"d\"] * 3], dtype=object).T\n",
      " |  >>> ohe = OneHotEncoder(max_categories=3, sparse=False).fit(X)\n",
      " |  >>> ohe.infrequent_categories_\n",
      " |  [array(['a', 'd'], dtype=object)]\n",
      " |  >>> ohe.transform([[\"a\"], [\"b\"]])\n",
      " |  array([[0., 0., 1.],\n",
      " |         [1., 0., 0.]])\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      OneHotEncoder\n",
      " |      _BaseEncoder\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, categories='auto', drop=None, sparse=True, dtype=<class 'numpy.float64'>, handle_unknown='error', min_frequency=None, max_categories=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None)\n",
      " |      Fit OneHotEncoder to X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          The data to determine the categories of each feature.\n",
      " |      \n",
      " |      y : None\n",
      " |          Ignored. This parameter exists only for compatibility with\n",
      " |          :class:`~sklearn.pipeline.Pipeline`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted encoder.\n",
      " |  \n",
      " |  fit_transform(self, X, y=None)\n",
      " |      Fit OneHotEncoder to X, then transform X.\n",
      " |      \n",
      " |      Equivalent to fit(X).transform(X) but more convenient.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          The data to encode.\n",
      " |      \n",
      " |      y : None\n",
      " |          Ignored. This parameter exists only for compatibility with\n",
      " |          :class:`~sklearn.pipeline.Pipeline`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_out : {ndarray, sparse matrix} of shape                 (n_samples, n_encoded_features)\n",
      " |          Transformed input. If `sparse=True`, a sparse matrix will be\n",
      " |          returned.\n",
      " |  \n",
      " |  get_feature_names(self, input_features=None)\n",
      " |      DEPRECATED: get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      " |      \n",
      " |      Return feature names for output features.\n",
      " |      \n",
      " |          For a given input feature, if there is an infrequent category, the most\n",
      " |          'infrequent_sklearn' will be used as a feature name.\n",
      " |      \n",
      " |          Parameters\n",
      " |          ----------\n",
      " |          input_features : list of str of shape (n_features,)\n",
      " |              String names for input features if available. By default,\n",
      " |              \"x0\", \"x1\", ... \"xn_features\" is used.\n",
      " |      \n",
      " |          Returns\n",
      " |          -------\n",
      " |          output_feature_names : ndarray of shape (n_output_features,)\n",
      " |              Array of feature names.\n",
      " |  \n",
      " |  get_feature_names_out(self, input_features=None)\n",
      " |      Get output feature names for transformation.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      input_features : array-like of str or None, default=None\n",
      " |          Input features.\n",
      " |      \n",
      " |          - If `input_features` is `None`, then `feature_names_in_` is\n",
      " |            used as feature names in. If `feature_names_in_` is not defined,\n",
      " |            then the following input feature names are generated:\n",
      " |            `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n",
      " |          - If `input_features` is an array-like, then `input_features` must\n",
      " |            match `feature_names_in_` if `feature_names_in_` is defined.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_names_out : ndarray of str objects\n",
      " |          Transformed feature names.\n",
      " |  \n",
      " |  inverse_transform(self, X)\n",
      " |      Convert the data back to the original representation.\n",
      " |      \n",
      " |      When unknown categories are encountered (all zeros in the\n",
      " |      one-hot encoding), ``None`` is used to represent this category. If the\n",
      " |      feature with the unknown category has a dropped category, the dropped\n",
      " |      category will be its inverse.\n",
      " |      \n",
      " |      For a given input feature, if there is an infrequent category,\n",
      " |      'infrequent_sklearn' will be used to represent the infrequent category.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape                 (n_samples, n_encoded_features)\n",
      " |          The transformed data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_tr : ndarray of shape (n_samples, n_features)\n",
      " |          Inverse transformed array.\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Transform X using one-hot encoding.\n",
      " |      \n",
      " |      If there are infrequent categories for a feature, the infrequent\n",
      " |      categories will be grouped into a single category.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          The data to encode.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_out : {ndarray, sparse matrix} of shape                 (n_samples, n_encoded_features)\n",
      " |          Transformed input. If `sparse=True`, a sparse matrix will be\n",
      " |          returned.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  infrequent_categories_\n",
      " |      Infrequent categories for each feature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "help(OneHotEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categories: [array(['Female', 'Male', 'Unknown'], dtype=object), array(['Chrome', 'Internet Explorer', 'Safari'], dtype=object)]\n",
      "feature names: ['gender_Female' 'gender_Male' 'gender_Unknown' 'browser_Chrome'\n",
      " 'browser_Internet Explorer' 'browser_Safari']\n",
      "X_train transformed\n",
      "  (0, 1)\t1.0\n",
      "  (0, 5)\t1.0\n",
      "  (1, 0)\t1.0\n",
      "  (1, 5)\t1.0\n",
      "  (2, 2)\t1.0\n",
      "  (2, 4)\t1.0\n",
      "  (3, 1)\t1.0\n",
      "  (3, 3)\t1.0\n",
      "  (4, 0)\t1.0\n",
      "  (4, 3)\t1.0\n",
      "  (5, 0)\t1.0\n",
      "  (5, 4)\t1.0\n",
      "X_test transformed\n",
      "  (0, 0)\t1.0\n",
      "  (0, 3)\t1.0\n",
      "  (1, 1)\t1.0\n",
      "  (2, 2)\t1.0\n",
      "  (2, 4)\t1.0\n",
      "  (3, 0)\t1.0\n",
      "  (3, 5)\t1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/azsom/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# toy example\n",
    "train = {'gender':['Male','Female','Unknown','Male','Female','Female'],\\\n",
    "         'browser':['Safari','Safari','Internet Explorer','Chrome','Chrome','Internet Explorer']}\n",
    "test = {'gender':['Female','Male','Unknown','Female'],'browser':['Chrome','Firefox','Internet Explorer','Safari']}\n",
    "\n",
    "Xtoy_train = pd.DataFrame(train)\n",
    "Xtoy_test = pd.DataFrame(test)\n",
    "\n",
    "ftrs = ['gender','browser']\n",
    "\n",
    "# initialize the encoder\n",
    "enc = OneHotEncoder(sparse=False) # by default, OneHotEncoder returns a sparse matrix. sparse=False returns a 2D array\n",
    "# fit the training data\n",
    "enc.fit(Xtoy_train)\n",
    "print('categories:',enc.categories_)\n",
    "print('feature names:',enc.get_feature_names(ftrs))\n",
    "# transform X_train\n",
    "X_train_ohe = enc.transform(Xtoy_train)\n",
    "#print(X_train_ohe)\n",
    "# do all of this in one step\n",
    "X_train_ohe = enc.fit_transform(Xtoy_train)\n",
    "print('X_train transformed')\n",
    "print(X_train_ohe)\n",
    "\n",
    "# transform X_test\n",
    "X_test_ohe = enc.transform(Xtoy_test)\n",
    "print('X_test transformed')\n",
    "print(X_test_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature names: ['workclass_ ?' 'workclass_ Federal-gov' 'workclass_ Local-gov'\n",
      " 'workclass_ Never-worked' 'workclass_ Private' 'workclass_ Self-emp-inc'\n",
      " 'workclass_ Self-emp-not-inc' 'workclass_ State-gov'\n",
      " 'workclass_ Without-pay' 'marital-status_ Divorced'\n",
      " 'marital-status_ Married-AF-spouse' 'marital-status_ Married-civ-spouse'\n",
      " 'marital-status_ Married-spouse-absent' 'marital-status_ Never-married'\n",
      " 'marital-status_ Separated' 'marital-status_ Widowed' 'occupation_ ?'\n",
      " 'occupation_ Adm-clerical' 'occupation_ Armed-Forces'\n",
      " 'occupation_ Craft-repair' 'occupation_ Exec-managerial'\n",
      " 'occupation_ Farming-fishing' 'occupation_ Handlers-cleaners'\n",
      " 'occupation_ Machine-op-inspct' 'occupation_ Other-service'\n",
      " 'occupation_ Priv-house-serv' 'occupation_ Prof-specialty'\n",
      " 'occupation_ Protective-serv' 'occupation_ Sales'\n",
      " 'occupation_ Tech-support' 'occupation_ Transport-moving'\n",
      " 'relationship_ Husband' 'relationship_ Not-in-family'\n",
      " 'relationship_ Other-relative' 'relationship_ Own-child'\n",
      " 'relationship_ Unmarried' 'relationship_ Wife' 'race_ Amer-Indian-Eskimo'\n",
      " 'race_ Asian-Pac-Islander' 'race_ Black' 'race_ Other' 'race_ White'\n",
      " 'sex_ Female' 'sex_ Male' 'native-country_ ?' 'native-country_ Cambodia'\n",
      " 'native-country_ Canada' 'native-country_ China'\n",
      " 'native-country_ Columbia' 'native-country_ Cuba'\n",
      " 'native-country_ Dominican-Republic' 'native-country_ Ecuador'\n",
      " 'native-country_ El-Salvador' 'native-country_ England'\n",
      " 'native-country_ France' 'native-country_ Germany'\n",
      " 'native-country_ Greece' 'native-country_ Guatemala'\n",
      " 'native-country_ Haiti' 'native-country_ Holand-Netherlands'\n",
      " 'native-country_ Honduras' 'native-country_ Hong'\n",
      " 'native-country_ Hungary' 'native-country_ India' 'native-country_ Iran'\n",
      " 'native-country_ Ireland' 'native-country_ Italy'\n",
      " 'native-country_ Jamaica' 'native-country_ Japan' 'native-country_ Laos'\n",
      " 'native-country_ Mexico' 'native-country_ Nicaragua'\n",
      " 'native-country_ Outlying-US(Guam-USVI-etc)' 'native-country_ Peru'\n",
      " 'native-country_ Philippines' 'native-country_ Poland'\n",
      " 'native-country_ Portugal' 'native-country_ Puerto-Rico'\n",
      " 'native-country_ Scotland' 'native-country_ South'\n",
      " 'native-country_ Taiwan' 'native-country_ Thailand'\n",
      " 'native-country_ Trinadad&Tobago' 'native-country_ United-States'\n",
      " 'native-country_ Vietnam' 'native-country_ Yugoslavia']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/azsom/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# apply OHE to the adult dataset\n",
    "\n",
    "# let's collect all categorical features first\n",
    "onehot_ftrs = ['workclass','marital-status','occupation','relationship','race','sex','native-country']\n",
    "# initialize the encoder\n",
    "enc = OneHotEncoder(sparse=False,handle_unknown='ignore') # by default, OneHotEncoder returns a sparse matrix. sparse=False returns a 2D array\n",
    "# fit the training data\n",
    "enc.fit(X_train[onehot_ftrs])\n",
    "print('feature names:',enc.get_feature_names(onehot_ftrs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformed train features:\n",
      "[[0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]]\n",
      "transformed val features:\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]]\n",
      "transformed test features:\n",
      "[[0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# transform X_train\n",
    "onehot_train = enc.transform(X_train[onehot_ftrs])\n",
    "print('transformed train features:')\n",
    "print(onehot_train)\n",
    "# transform X_val\n",
    "onehot_val = enc.transform(X_val[onehot_ftrs])\n",
    "print('transformed val features:')\n",
    "print(onehot_val)\n",
    "# transform X_test\n",
    "onehot_test = enc.transform(X_test[onehot_ftrs])\n",
    "print('transformed test features:')\n",
    "print(onehot_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quiz 1\n",
    "Please explain how you would encode the race feature below and what would be the output of the encoder. Do not write code. The goal of this quiz is to test your conceptual understanding so write text and the output array.\n",
    "\n",
    "race = [' Amer-Indian-Eskimo', 'White', 'Black', 'Asian-Pac-Islander', 'Black', 'White', 'White']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <font color='LIGHTGRAY'>By the end of this lecture, you will be able to</font>\n",
    "- <font color='LIGHTGRAY'>apply one-hot encoding or ordinal encoding to categorical variables</font>\n",
    "- **apply scaling and normalization to continuous variables**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Continuous features: MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If the continuous feature values are reasonably bounded, MinMaxScaler is a good way to scale the features.\n",
    "- Age is expected to be within the range of 0 and 100.\n",
    "- Number of hours worked per week is in the range of 0 to 80.\n",
    "- If unsure, plot the histogram of the feature to verify or just go with the standard scaler!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class MinMaxScaler in module sklearn.preprocessing._data:\n",
      "\n",
      "class MinMaxScaler(sklearn.base._OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      " |  MinMaxScaler(feature_range=(0, 1), *, copy=True, clip=False)\n",
      " |  \n",
      " |  Transform features by scaling each feature to a given range.\n",
      " |  \n",
      " |  This estimator scales and translates each feature individually such\n",
      " |  that it is in the given range on the training set, e.g. between\n",
      " |  zero and one.\n",
      " |  \n",
      " |  The transformation is given by::\n",
      " |  \n",
      " |      X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
      " |      X_scaled = X_std * (max - min) + min\n",
      " |  \n",
      " |  where min, max = feature_range.\n",
      " |  \n",
      " |  This transformation is often used as an alternative to zero mean,\n",
      " |  unit variance scaling.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <preprocessing_scaler>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  feature_range : tuple (min, max), default=(0, 1)\n",
      " |      Desired range of transformed data.\n",
      " |  \n",
      " |  copy : bool, default=True\n",
      " |      Set to False to perform inplace row normalization and avoid a\n",
      " |      copy (if the input is already a numpy array).\n",
      " |  \n",
      " |  clip : bool, default=False\n",
      " |      Set to True to clip transformed values of held-out data to\n",
      " |      provided `feature range`.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  min_ : ndarray of shape (n_features,)\n",
      " |      Per feature adjustment for minimum. Equivalent to\n",
      " |      ``min - X.min(axis=0) * self.scale_``\n",
      " |  \n",
      " |  scale_ : ndarray of shape (n_features,)\n",
      " |      Per feature relative scaling of the data. Equivalent to\n",
      " |      ``(max - min) / (X.max(axis=0) - X.min(axis=0))``\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *scale_* attribute.\n",
      " |  \n",
      " |  data_min_ : ndarray of shape (n_features,)\n",
      " |      Per feature minimum seen in the data\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *data_min_*\n",
      " |  \n",
      " |  data_max_ : ndarray of shape (n_features,)\n",
      " |      Per feature maximum seen in the data\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *data_max_*\n",
      " |  \n",
      " |  data_range_ : ndarray of shape (n_features,)\n",
      " |      Per feature range ``(data_max_ - data_min_)`` seen in the data\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *data_range_*\n",
      " |  \n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  n_samples_seen_ : int\n",
      " |      The number of samples processed by the estimator.\n",
      " |      It will be reset on new calls to fit, but increments across\n",
      " |      ``partial_fit`` calls.\n",
      " |  \n",
      " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      " |      has feature names that are all strings.\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  minmax_scale : Equivalent function without the estimator API.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  NaNs are treated as missing values: disregarded in fit, and maintained in\n",
      " |  transform.\n",
      " |  \n",
      " |  For a comparison of the different scalers, transformers, and normalizers,\n",
      " |  see :ref:`examples/preprocessing/plot_all_scaling.py\n",
      " |  <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.preprocessing import MinMaxScaler\n",
      " |  >>> data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n",
      " |  >>> scaler = MinMaxScaler()\n",
      " |  >>> print(scaler.fit(data))\n",
      " |  MinMaxScaler()\n",
      " |  >>> print(scaler.data_max_)\n",
      " |  [ 1. 18.]\n",
      " |  >>> print(scaler.transform(data))\n",
      " |  [[0.   0.  ]\n",
      " |   [0.25 0.25]\n",
      " |   [0.5  0.5 ]\n",
      " |   [1.   1.  ]]\n",
      " |  >>> print(scaler.transform([[2, 2]]))\n",
      " |  [[1.5 0. ]]\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      MinMaxScaler\n",
      " |      sklearn.base._OneToOneFeatureMixin\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, feature_range=(0, 1), *, copy=True, clip=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None)\n",
      " |      Compute the minimum and maximum to be used for later scaling.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          The data used to compute the per-feature minimum and maximum\n",
      " |          used for later scaling along the features axis.\n",
      " |      \n",
      " |      y : None\n",
      " |          Ignored.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Fitted scaler.\n",
      " |  \n",
      " |  inverse_transform(self, X)\n",
      " |      Undo the scaling of X according to feature_range.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Input data that will be transformed. It cannot be sparse.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Xt : ndarray of shape (n_samples, n_features)\n",
      " |          Transformed data.\n",
      " |  \n",
      " |  partial_fit(self, X, y=None)\n",
      " |      Online computation of min and max on X for later scaling.\n",
      " |      \n",
      " |      All of X is processed as a single batch. This is intended for cases\n",
      " |      when :meth:`fit` is not feasible due to very large number of\n",
      " |      `n_samples` or because X is read from a continuous stream.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          The data used to compute the mean and standard deviation\n",
      " |          used for later scaling along the features axis.\n",
      " |      \n",
      " |      y : None\n",
      " |          Ignored.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Fitted scaler.\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Scale features of X according to feature_range.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Input data that will be transformed.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Xt : ndarray of shape (n_samples, n_features)\n",
      " |          Transformed data.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base._OneToOneFeatureMixin:\n",
      " |  \n",
      " |  get_feature_names_out(self, input_features=None)\n",
      " |      Get output feature names for transformation.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      input_features : array-like of str or None, default=None\n",
      " |          Input features.\n",
      " |      \n",
      " |          - If `input_features` is `None`, then `feature_names_in_` is\n",
      " |            used as feature names in. If `feature_names_in_` is not defined,\n",
      " |            then the following input feature names are generated:\n",
      " |            `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n",
      " |          - If `input_features` is an array-like, then `input_features` must\n",
      " |            match `feature_names_in_` if `feature_names_in_` is defined.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_names_out : ndarray of str objects\n",
      " |          Same as input features.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base._OneToOneFeatureMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  fit_transform(self, X, y=None, **fit_params)\n",
      " |      Fit to data, then transform it.\n",
      " |      \n",
      " |      Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      " |      and returns a transformed version of `X`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Input samples.\n",
      " |      \n",
      " |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      " |          Target values (None for unsupervised transformations).\n",
      " |      \n",
      " |      **fit_params : dict\n",
      " |          Additional fit parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      " |          Transformed array.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "help(MinMaxScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.30645161 0.        ]\n",
      " [0.83870968 0.66666667]\n",
      " [0.         0.16666667]\n",
      " [0.88709677 1.        ]\n",
      " [0.46774194 0.66666667]\n",
      " [1.         0.33333333]\n",
      " [0.30645161 0.66666667]]\n",
      "[[ 1.12903226  0.        ]\n",
      " [ 0.20967742  0.66666667]\n",
      " [-0.0483871   0.        ]\n",
      " [ 0.75806452  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# toy data\n",
    "# let's assume we have two continuous features:\n",
    "train = {'age':[32,65,13,68,42,75,32],'number of hours worked':[0,40,10,60,40,20,40]}\n",
    "test = {'age':[83,26,10,60],'number of hours worked':[0,40,0,60]}\n",
    "\n",
    "# (value - min) / (max - min), if value is 32, min is 13 and max is 75, then we have 19 / 62 = 0.3064\n",
    "\n",
    "Xtoy_train = pd.DataFrame(train)\n",
    "Xtoy_test = pd.DataFrame(test)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(Xtoy_train)\n",
    "print(scaler.transform(Xtoy_train))\n",
    "print(scaler.transform(Xtoy_test)) # note how scaled X_test contains values larger than 1 and smaller than 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.19178082 0.39795918]\n",
      " [0.32876712 0.39795918]\n",
      " [0.60273973 0.5       ]\n",
      " ...\n",
      " [0.01369863 0.19387755]\n",
      " [0.45205479 0.84693878]\n",
      " [0.23287671 0.60204082]]\n",
      "[[0.35616438 0.5       ]\n",
      " [0.68493151 0.39795918]\n",
      " [0.09589041 0.39795918]\n",
      " ...\n",
      " [0.09589041 0.19387755]\n",
      " [0.02739726 0.44897959]\n",
      " [0.38356164 0.39795918]]\n",
      "[[0.06849315 0.39795918]\n",
      " [0.23287671 0.39795918]\n",
      " [0.43835616 0.5       ]\n",
      " ...\n",
      " [0.20547945 0.39795918]\n",
      " [0.21917808 0.37755102]\n",
      " [0.08219178 0.35714286]]\n"
     ]
    }
   ],
   "source": [
    "# adult data\n",
    "\n",
    "minmax_ftrs = ['age','hours-per-week']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train[minmax_ftrs])\n",
    "print(scaler.transform(X_train[minmax_ftrs]))\n",
    "print(scaler.transform(X_val[minmax_ftrs])) \n",
    "print(scaler.transform(X_test[minmax_ftrs])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Continuous features: StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If the continuous feature values follow a tailed distribution, StandardScaler is better to use!\n",
    "- Salaries are a good example. Most people earn less than 100k but there are a small number of super-rich people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class StandardScaler in module sklearn.preprocessing._data:\n",
      "\n",
      "class StandardScaler(sklearn.base._OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      " |  StandardScaler(*, copy=True, with_mean=True, with_std=True)\n",
      " |  \n",
      " |  Standardize features by removing the mean and scaling to unit variance.\n",
      " |  \n",
      " |  The standard score of a sample `x` is calculated as:\n",
      " |  \n",
      " |      z = (x - u) / s\n",
      " |  \n",
      " |  where `u` is the mean of the training samples or zero if `with_mean=False`,\n",
      " |  and `s` is the standard deviation of the training samples or one if\n",
      " |  `with_std=False`.\n",
      " |  \n",
      " |  Centering and scaling happen independently on each feature by computing\n",
      " |  the relevant statistics on the samples in the training set. Mean and\n",
      " |  standard deviation are then stored to be used on later data using\n",
      " |  :meth:`transform`.\n",
      " |  \n",
      " |  Standardization of a dataset is a common requirement for many\n",
      " |  machine learning estimators: they might behave badly if the\n",
      " |  individual features do not more or less look like standard normally\n",
      " |  distributed data (e.g. Gaussian with 0 mean and unit variance).\n",
      " |  \n",
      " |  For instance many elements used in the objective function of\n",
      " |  a learning algorithm (such as the RBF kernel of Support Vector\n",
      " |  Machines or the L1 and L2 regularizers of linear models) assume that\n",
      " |  all features are centered around 0 and have variance in the same\n",
      " |  order. If a feature has a variance that is orders of magnitude larger\n",
      " |  than others, it might dominate the objective function and make the\n",
      " |  estimator unable to learn from other features correctly as expected.\n",
      " |  \n",
      " |  This scaler can also be applied to sparse CSR or CSC matrices by passing\n",
      " |  `with_mean=False` to avoid breaking the sparsity structure of the data.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <preprocessing_scaler>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  copy : bool, default=True\n",
      " |      If False, try to avoid a copy and do inplace scaling instead.\n",
      " |      This is not guaranteed to always work inplace; e.g. if the data is\n",
      " |      not a NumPy array or scipy.sparse CSR matrix, a copy may still be\n",
      " |      returned.\n",
      " |  \n",
      " |  with_mean : bool, default=True\n",
      " |      If True, center the data before scaling.\n",
      " |      This does not work (and will raise an exception) when attempted on\n",
      " |      sparse matrices, because centering them entails building a dense\n",
      " |      matrix which in common use cases is likely to be too large to fit in\n",
      " |      memory.\n",
      " |  \n",
      " |  with_std : bool, default=True\n",
      " |      If True, scale the data to unit variance (or equivalently,\n",
      " |      unit standard deviation).\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  scale_ : ndarray of shape (n_features,) or None\n",
      " |      Per feature relative scaling of the data to achieve zero mean and unit\n",
      " |      variance. Generally this is calculated using `np.sqrt(var_)`. If a\n",
      " |      variance is zero, we can't achieve unit variance, and the data is left\n",
      " |      as-is, giving a scaling factor of 1. `scale_` is equal to `None`\n",
      " |      when `with_std=False`.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *scale_*\n",
      " |  \n",
      " |  mean_ : ndarray of shape (n_features,) or None\n",
      " |      The mean value for each feature in the training set.\n",
      " |      Equal to ``None`` when ``with_mean=False``.\n",
      " |  \n",
      " |  var_ : ndarray of shape (n_features,) or None\n",
      " |      The variance for each feature in the training set. Used to compute\n",
      " |      `scale_`. Equal to ``None`` when ``with_std=False``.\n",
      " |  \n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      " |      has feature names that are all strings.\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  n_samples_seen_ : int or ndarray of shape (n_features,)\n",
      " |      The number of samples processed by the estimator for each feature.\n",
      " |      If there are no missing samples, the ``n_samples_seen`` will be an\n",
      " |      integer, otherwise it will be an array of dtype int. If\n",
      " |      `sample_weights` are used it will be a float (if no missing data)\n",
      " |      or an array of dtype float that sums the weights seen so far.\n",
      " |      Will be reset on new calls to fit, but increments across\n",
      " |      ``partial_fit`` calls.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  scale : Equivalent function without the estimator API.\n",
      " |  \n",
      " |  :class:`~sklearn.decomposition.PCA` : Further removes the linear\n",
      " |      correlation across features with 'whiten=True'.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  NaNs are treated as missing values: disregarded in fit, and maintained in\n",
      " |  transform.\n",
      " |  \n",
      " |  We use a biased estimator for the standard deviation, equivalent to\n",
      " |  `numpy.std(x, ddof=0)`. Note that the choice of `ddof` is unlikely to\n",
      " |  affect model performance.\n",
      " |  \n",
      " |  For a comparison of the different scalers, transformers, and normalizers,\n",
      " |  see :ref:`examples/preprocessing/plot_all_scaling.py\n",
      " |  <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.preprocessing import StandardScaler\n",
      " |  >>> data = [[0, 0], [0, 0], [1, 1], [1, 1]]\n",
      " |  >>> scaler = StandardScaler()\n",
      " |  >>> print(scaler.fit(data))\n",
      " |  StandardScaler()\n",
      " |  >>> print(scaler.mean_)\n",
      " |  [0.5 0.5]\n",
      " |  >>> print(scaler.transform(data))\n",
      " |  [[-1. -1.]\n",
      " |   [-1. -1.]\n",
      " |   [ 1.  1.]\n",
      " |   [ 1.  1.]]\n",
      " |  >>> print(scaler.transform([[2, 2]]))\n",
      " |  [[3. 3.]]\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      StandardScaler\n",
      " |      sklearn.base._OneToOneFeatureMixin\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, copy=True, with_mean=True, with_std=True)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None, sample_weight=None)\n",
      " |      Compute the mean and std to be used for later scaling.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The data used to compute the mean and standard deviation\n",
      " |          used for later scaling along the features axis.\n",
      " |      \n",
      " |      y : None\n",
      " |          Ignored.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Individual weights for each sample.\n",
      " |      \n",
      " |          .. versionadded:: 0.24\n",
      " |             parameter *sample_weight* support to StandardScaler.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Fitted scaler.\n",
      " |  \n",
      " |  inverse_transform(self, X, copy=None)\n",
      " |      Scale back the data to the original representation.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The data used to scale along the features axis.\n",
      " |      copy : bool, default=None\n",
      " |          Copy the input X or not.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Transformed array.\n",
      " |  \n",
      " |  partial_fit(self, X, y=None, sample_weight=None)\n",
      " |      Online computation of mean and std on X for later scaling.\n",
      " |      \n",
      " |      All of X is processed as a single batch. This is intended for cases\n",
      " |      when :meth:`fit` is not feasible due to very large number of\n",
      " |      `n_samples` or because X is read from a continuous stream.\n",
      " |      \n",
      " |      The algorithm for incremental mean and std is given in Equation 1.5a,b\n",
      " |      in Chan, Tony F., Gene H. Golub, and Randall J. LeVeque. \"Algorithms\n",
      " |      for computing the sample variance: Analysis and recommendations.\"\n",
      " |      The American Statistician 37.3 (1983): 242-247:\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The data used to compute the mean and standard deviation\n",
      " |          used for later scaling along the features axis.\n",
      " |      \n",
      " |      y : None\n",
      " |          Ignored.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Individual weights for each sample.\n",
      " |      \n",
      " |          .. versionadded:: 0.24\n",
      " |             parameter *sample_weight* support to StandardScaler.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Fitted scaler.\n",
      " |  \n",
      " |  transform(self, X, copy=None)\n",
      " |      Perform standardization by centering and scaling.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix of shape (n_samples, n_features)\n",
      " |          The data used to scale along the features axis.\n",
      " |      copy : bool, default=None\n",
      " |          Copy the input X or not.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Transformed array.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base._OneToOneFeatureMixin:\n",
      " |  \n",
      " |  get_feature_names_out(self, input_features=None)\n",
      " |      Get output feature names for transformation.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      input_features : array-like of str or None, default=None\n",
      " |          Input features.\n",
      " |      \n",
      " |          - If `input_features` is `None`, then `feature_names_in_` is\n",
      " |            used as feature names in. If `feature_names_in_` is not defined,\n",
      " |            then the following input feature names are generated:\n",
      " |            `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n",
      " |          - If `input_features` is an array-like, then `input_features` must\n",
      " |            match `feature_names_in_` if `feature_names_in_` is defined.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_names_out : ndarray of str objects\n",
      " |          Same as input features.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base._OneToOneFeatureMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  fit_transform(self, X, y=None, **fit_params)\n",
      " |      Fit to data, then transform it.\n",
      " |      \n",
      " |      Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      " |      and returns a transformed version of `X`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Input samples.\n",
      " |      \n",
      " |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      " |          Target values (None for unsupervised transformations).\n",
      " |      \n",
      " |      **fit_params : dict\n",
      " |          Additional fit parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      " |          Transformed array.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "help(StandardScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.44873188]\n",
      " [-0.36895732]\n",
      " [-0.4806417 ]\n",
      " [ 2.58270127]\n",
      " [-0.51255153]\n",
      " [ 0.18946457]\n",
      " [-0.49659661]\n",
      " [-0.46468679]]\n",
      "[[-0.52850644]\n",
      " [-0.43277697]\n",
      " [ 4.1781924 ]\n",
      " [-0.41682206]]\n"
     ]
    }
   ],
   "source": [
    "# toy data\n",
    "train = {'salary':[50_000,75_000,40_000,1_000_000,30_000,250_000,35_000,45_000]}\n",
    "test = {'salary':[25_000,55_000,1_500_000,60_000]}\n",
    "\n",
    "Xtoy_train = pd.DataFrame(train)\n",
    "Xtoy_test = pd.DataFrame(test)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "print(scaler.fit_transform(Xtoy_train))\n",
    "print(scaler.transform(Xtoy_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.14633293 -0.22318878]\n",
      " [-0.14633293 -0.22318878]\n",
      " [-0.14633293 -0.22318878]\n",
      " ...\n",
      " [-0.14633293 -0.22318878]\n",
      " [-0.14633293 -0.22318878]\n",
      " [-0.14633293 -0.22318878]]\n",
      "[[-0.14633293 -0.22318878]\n",
      " [-0.14633293 -0.22318878]\n",
      " [-0.14633293 -0.22318878]\n",
      " ...\n",
      " [-0.14633293 -0.22318878]\n",
      " [-0.14633293 -0.22318878]\n",
      " [-0.14633293 -0.22318878]]\n",
      "[[-0.14633293 -0.22318878]\n",
      " [-0.14633293 -0.22318878]\n",
      " [-0.14633293 -0.22318878]\n",
      " ...\n",
      " [-0.14633293 -0.22318878]\n",
      " [-0.14633293 -0.22318878]\n",
      " [-0.14633293 -0.22318878]]\n"
     ]
    }
   ],
   "source": [
    "# adult data\n",
    "\n",
    "std_ftrs = ['capital-gain','capital-loss']\n",
    "scaler = StandardScaler()\n",
    "print(scaler.fit_transform(X_train[std_ftrs]))\n",
    "print(scaler.transform(X_val[std_ftrs]))\n",
    "print(scaler.transform(X_test[std_ftrs]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quiz 2\n",
    "\n",
    "Which of these features could be safely preprocessed by the minmax scaler?\n",
    "- number of minutes spent on the website in a day\n",
    "- number of days a year spent abroad in a year\n",
    "- USD donated to charity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How and when to do preprocessing in the ML pipeline?\n",
    "\n",
    "- **APPLY TRANSFORMER.FIT ONLY ON YOUR TRAINING DATA!** Then transform the validation and test sets.\n",
    "- One of the most common mistake practitioners make is leaking statistics!\n",
    "     - fit_transform is applied to the whole dataset, then the data is split into train/validation/test\n",
    "         - this is wrong because the test set statistics impacts how the training and validation sets are transformed\n",
    "         - but the test set must be separated by train and val, and val must be separated by train\n",
    "     - or fit_transform is applied to the train, then fit_transform is applied to the validation set, and fit_transform is applied to the test set\n",
    "         - this is wrong because the relative position of the points change\n",
    "<center><img src=\"figures/no_separate_scaling.png\" width=\"1200\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Scikit-learn's pipelines\n",
    "\n",
    "- The steps in the ML pipleine can be chained together into a scikit-learn pipeline which consists of transformers and one final estimator which is usually your classifier or regression model.\n",
    "- It neatly combines the preprocessing steps and it helps to avoid leaking statistics.\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#np.random.seed(0)\n",
    "\n",
    "df = pd.read_csv('data/adult_data.csv')\n",
    "\n",
    "# let's separate the feature matrix X, and target variable y\n",
    "y = df['gross-income'] # remember, we want to predict who earns more than 50k or less than 50k\n",
    "X = df.loc[:, df.columns != 'gross-income'] # all other columns are features\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "# first split to separate out the training set\n",
    "X_train, X_other, y_train, y_other = train_test_split(X,y,train_size = 0.6,random_state=random_state)\n",
    "\n",
    "# second split to separate out the validation and test sets\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_other,y_other,train_size = 0.5,random_state=random_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19536, 14)\n",
      "(19536, 91)\n",
      "[[10.          0.          0.         ...  0.39795918 -0.14633293\n",
      "  -0.22318878]\n",
      " [ 9.          0.          0.         ...  0.39795918 -0.14633293\n",
      "  -0.22318878]\n",
      " [ 8.          0.          0.         ...  0.5        -0.14633293\n",
      "  -0.22318878]\n",
      " ...\n",
      " [ 6.          0.          0.         ...  0.19387755 -0.14633293\n",
      "  -0.22318878]\n",
      " [ 8.          0.          0.         ...  0.84693878 -0.14633293\n",
      "  -0.22318878]\n",
      " [12.          0.          0.         ...  0.60204082 -0.14633293\n",
      "  -0.22318878]]\n"
     ]
    }
   ],
   "source": [
    "# collect which encoder to use on each feature\n",
    "# needs to be done manually\n",
    "ordinal_ftrs = ['education'] \n",
    "ordinal_cats = [[' Preschool',' 1st-4th',' 5th-6th',' 7th-8th',' 9th',' 10th',' 11th',' 12th',' HS-grad',\\\n",
    "                ' Some-college',' Assoc-voc',' Assoc-acdm',' Bachelors',' Masters',' Prof-school',' Doctorate']]\n",
    "onehot_ftrs = ['workclass','marital-status','occupation','relationship','race','sex','native-country']\n",
    "minmax_ftrs = ['age','hours-per-week']\n",
    "std_ftrs = ['capital-gain','capital-loss']\n",
    "\n",
    "# collect all the encoders\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('ord', OrdinalEncoder(categories = ordinal_cats), ordinal_ftrs),\n",
    "        ('onehot', OneHotEncoder(sparse=False,handle_unknown='ignore'), onehot_ftrs),\n",
    "        ('minmax', MinMaxScaler(), minmax_ftrs),\n",
    "        ('std', StandardScaler(), std_ftrs)])\n",
    "\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor)]) # for now we only preprocess \n",
    "                                                       # later on we will add other steps here\n",
    "\n",
    "X_train_prep = clf.fit_transform(X_train)\n",
    "X_val_prep = clf.transform(X_val)\n",
    "X_test_prep = clf.transform(X_test)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_train_prep.shape)\n",
    "print(X_train_prep)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mudcard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
