{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mudcard\n",
    "- **can this method be used to explain outliers?**\n",
    "    - any point in a supervised ML problem can be explained by shap\n",
    "- **For a linear model, will the shap values be the same for each data point? Since it is determined by the coefficients of the equation.**\n",
    "    - nope\n",
    "    - the coefficients are the same for each feature, but the feature values change for each point and so does the shap values\n",
    "- **Is SHAP appropriate for multi-class classification problems? Or is it too computationally expensive?**\n",
    "    - it works with multi-class problems\n",
    "    - I wouldn't exclude a method just because it is computationally expesive\n",
    "- **Are you basically saying that mean(|SHAP value|) is another metric to measure global importance?**\n",
    "    - yes! :)\n",
    "- **I have difficulty understanding the plots of feature value vs. shap value. How should we interpret them?**\n",
    "    - play around with the code, make sure you understand every line\n",
    "    - read the linked papers and books\n",
    "- **Do we have a general sheet for the formulas that we have learnt in this course for the final exam?**\n",
    "    - not really\n",
    "    - it's an open-book exam so I won't ask questions you can simply copy-paste from a sheet or the lecture notes\n",
    "    - the exam tests your understanding of ML concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A note on imbalanced datasets\n",
    "- we learnt that a classification problem is imbalanced if more than 90-95% of the points belong to one class (class 0) and only a small fraction of the points belong to the other class (class 1)\n",
    "    - fraud detection\n",
    "    - sick or not sick (usually by far most people are not sick)\n",
    "- we learnt to not use a metric that relies on the True Negatives in the confusion matrix\n",
    "    - no accuracy or ROC\n",
    "    - use f_beta or the precision-recall curve instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What else can I do if I have an imbalanced dataset?\n",
    "- most (but not all) classification algorithms we covered have a parameter called `class_weight` which allows you to assign more weight to the class 1 point\n",
    "    - a misclassified class 1 point will contribute more to the cost function than a misclassified class 0 point\n",
    "    - read the manual on `class_weight` because the different algorithms have slightly different definitions for this parameter\n",
    "    - usually you can use `None`, `balanced`, or manually define what the class weight should be\n",
    "    - it is worthwhile to tune this parameter if you have an imbalanced dataset\n",
    "- resample/augment the dataset\n",
    "    - SMOTE (Synthetic Minority Over-sampling Technique), see the [paper](https://arxiv.org/abs/1106.1813)\n",
    "    - to improve the balance of the problem, new class 1 examples are synthesized from the existing examples\n",
    "    - be careful though!\n",
    "        - while resampling improves the balance of the dataset, the results of the model can be misleading\n",
    "        - when you deploy the model, the incoming data will be as imbalanced as the original data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misleading results with resampling\n",
    "- let's assume you have an imbalanced dataset with 99% of points in class 0 and 1% of points in class 1\n",
    "- you resample it such that the improved class balance is 50-50\n",
    "- here the confusion matrix of the trained model:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td colspan=\"2\" rowspan=\"2\"></td>\n",
    "        <td colspan=\"2\">Predicted class</td>\t\t\t\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Predicted Negative (0)</td>\n",
    "        <td>Predicted Positive (1)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td rowspan=\"2\">Actual class</td>\n",
    "        <td>Condition Negative (0)</td>\n",
    "        <td><b>True Negative (TN): 45%</b></td>\n",
    "        <td><b>False Positive (FP): 5%</b></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Condition Positive (1)</td>\n",
    "        <td><b>False Negative (FN): 5%</b></td>\n",
    "        <td><b>True Positive (TP): 45%</b></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "- 90% accuracy which is well above the 50% baseline accuracy!\n",
    "- the precision, recall, and f1 scores are all 0.9.\n",
    "- it looks great, doesn't it?\n",
    "- let's rewrite the confusion matrix to reflect rates with respect to the Condition Negative and Condition Positive points!\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td colspan=\"2\" rowspan=\"2\"></td>\n",
    "        <td colspan=\"2\">Predicted class</td>\t\t\t\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Predicted Negative (0)</td>\n",
    "        <td>Predicted Positive (1)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td rowspan=\"2\">Actual class</td>\n",
    "        <td>Condition Negative (0) 50% of the points</td>\n",
    "        <td><b>90% of CNs are correctly classified</b></td>\n",
    "        <td><b>10% of CNs are incorrectly classified</b></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Condition Positive (1) 50% of the points</td>\n",
    "        <td><b>10% of CPs are incorrectly classified</b></td>\n",
    "        <td><b>90% of CPs are correctly classified</b></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's deploy this model\n",
    "\n",
    "- the incoming data has the same balance as the original dataset (99% to 1%)\n",
    "- let's assume we have 1e5 new points, 9.9e4 belongs to class 0, 1000 belongs to class 1\n",
    "- what will be the numbers in the confusion matrix?\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td colspan=\"2\" rowspan=\"2\"></td>\n",
    "        <td colspan=\"2\">Predicted class</td>\t\t\t\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Predicted Negative (0)</td>\n",
    "        <td>Predicted Positive (1)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td rowspan=\"2\">Actual class</td>\n",
    "        <td>Condition Negative (0) 99000 points</td>\n",
    "        <td><b>True Negative (TN): 99000 * 0.9 = 89100 </b></td>\n",
    "        <td><b>False Positive (FP): 99000 * 0.1 = 9900 </b></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Condition Positive (1) 1000 points</td>\n",
    "        <td><b>False Negative (FN): 1000 * 0.1 = 100</b></td>\n",
    "        <td><b>True Positive (TP): 1000 * 0.9 = 900</b></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "- the accuracy of this model is still 0.90 but now it is well below the baseline of 0.99!\n",
    "- recall is good (0.90) but the precision is not great (~0.083)\n",
    "- the f1 score is ~0.15\n",
    "- the false positives are overwhelming\n",
    "- this is why you need to be careful with resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mudcard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
