{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mudcard\n",
    "- **How do we find the range for C and gamma for hyperparameter tuning of SVMs? Do we need to visualize the data first?**\n",
    "- **For SVR, how do we know what the width of the gaussian should be? Is it better if the width is as low as possible?**\n",
    "    - it's a hyperparameter (gamma) so as usual, you calculate train and validation scores\n",
    "- **In the case of regression, are SVM's exactly the same as kernel density estimation?**\n",
    "    - it's very similar but the goal is different\n",
    "    - in KDE, your goal is to plot a smooth distribution instead of a histogram\n",
    "    - in SVR rbf, your goal is to predict the regression target variable for previously unseen points\n",
    "  \n",
    "\n",
    "- **Muddiest part was understanding how summing different gaussian functions result in the final prediction function for SVR. Is this summing similar to the Taylor series of a function?**\n",
    "    - nope\n",
    "    - it's quite literally just replacing each point with a gaussian and the model prediction is the sum of the gaussians\n",
    "- **I am still confused how widening the Gaussian predictions creates such a smooth curve for predictions in SVMs.**\n",
    "    - implement the algorithm yourself to figure it out\n",
    "    - it's not too difficult and it's a great exercise to deepen your understanding\n",
    "- **Which library or method would you recommend if we want to check the memory used?**\n",
    "    - as usual, ask [stackoverflow](https://stackoverflow.com/questions/110259/which-python-memory-profiler-is-recommended)\n",
    "- **Could you please post the codes for the quiz 2?**\n",
    "    - once you submit your solution, the code should show up in canvas\n",
    "- **I'm still unclear about the quiz question:¬†The random forest run-time scales linearly with n_samples? I couldn't tell the linear scaling from the graph or the run time values.**\n",
    "    - you might need to average the runtime of multiple fits or use more datapoints to see it well\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The supervised ML pipeline\n",
    "The goal: Use the training data (X and y) to develop a <font color='red'>model</font> which can <font color='red'>accurately</font> predict the target variable (y_new') for previously unseen data (X_new).\n",
    "\n",
    "**1. Exploratory Data Analysis (EDA)**: you need to understand your data and verify that it doesn't contain errors\n",
    "   - do as much EDA as you can!\n",
    "    \n",
    "**2. Split the data into different sets**: most often the sets are train, validation, and test (or holdout)\n",
    "   - practitioners often make errors in this step!\n",
    "   - you can split the data randomly, based on groups, based on time, or any other non-standard way if necessary to answer your ML question\n",
    "\n",
    "**3. Preprocess the data**: ML models only work if X and Y are numbers! Some ML models additionally require each feature to have 0 mean and 1 standard deviation (standardized features)\n",
    "   - often the original features you get contain strings (for example a gender feature would contain 'male', 'female', 'non-binary', 'unknown') which needs to transformed into numbers\n",
    "   - often the features are not standardized (e.g., age is between 0 and 100) but it needs to be standardized\n",
    "    \n",
    "**4. Choose an evaluation metric**: depends on the priorities of the stakeholders\n",
    "   - often requires quite a bit of thinking and ethical considerations\n",
    "     \n",
    "**5. Choose one or more ML techniques**: it is highly recommended that you try multiple models\n",
    "   - start with simple models like linear or logistic regression\n",
    "   - try also more complex models like nearest neighbors, support vector machines, random forest, etc.\n",
    "    \n",
    "<span style=\"background-color: #FFFF00\">**6. Tune the hyperparameters of your ML models (aka cross-validation)**</span>\n",
    "   - ML techniques have hyperparameters that you need to optimize to achieve best performance\n",
    "   - for each ML model, decide which parameters to tune and what values to try\n",
    "   - loop through each parameter combination\n",
    "       - train one model for each parameter combination\n",
    "       - evaluate how well the model performs on the validation set\n",
    "   - take the parameter combo that gives the best validation score\n",
    "   - evaluate that model on the test set to report how well the model is expected to perform on previously unseen data\n",
    "    \n",
    "**7. Interpret your model**: black boxes are often not useful\n",
    "   - check if your model uses features that make sense (excellent tool for debugging)\n",
    "   - often model predictions are not enough, you need to be able to explain how the model arrived to a particular prediction (e.g., in health care)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's put everything together\n",
    "- IID data first!\n",
    "- the adult dataset\n",
    "- the next two cells were copied from the week 3 material and slightly rewritten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import packages\n",
    "\n",
    "load your dataset\n",
    "\n",
    "create feature matrix and target variable\n",
    "\n",
    "for i in random_states:\n",
    "\n",
    "   - split the data\n",
    "   - preprocess it\n",
    "   - decide which hyperparameters you'll tune and what values you'll try\n",
    "   - for combo in hyperparameters:\n",
    "       - train your ML algo\n",
    "       - calculate validation scores\n",
    "   - select best model based on the mean and std validation scores\n",
    "   - predict the test set using the best model\n",
    "   - return your test score (generalization error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, MinMaxScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('data/adult_data.csv')\n",
    "\n",
    "# let's separate the feature matrix X, and target variable y\n",
    "y = df['gross-income'] # remember, we want to predict who earns more than 50k or less than 50k\n",
    "X = df.loc[:, df.columns != 'gross-income'] # all other columns are features\n",
    "\n",
    "# collect which encoder to use on each feature\n",
    "# needs to be done manually\n",
    "ordinal_ftrs = ['education'] \n",
    "ordinal_cats = [[' Preschool',' 1st-4th',' 5th-6th',' 7th-8th',' 9th',' 10th',' 11th',' 12th',' HS-grad',\\\n",
    "                ' Some-college',' Assoc-voc',' Assoc-acdm',' Bachelors',' Masters',' Prof-school',' Doctorate']]\n",
    "onehot_ftrs = ['workclass','marital-status','occupation','relationship','race','sex','native-country']\n",
    "minmax_ftrs = ['age','hours-per-week']\n",
    "std_ftrs = ['capital-gain','capital-loss']\n",
    "\n",
    "# collect all the encoders into one preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('ord', OrdinalEncoder(categories = ordinal_cats), ordinal_ftrs),\n",
    "        ('onehot', OneHotEncoder(sparse=False,handle_unknown='ignore'), onehot_ftrs),\n",
    "        ('minmax', MinMaxScaler(), minmax_ftrs),\n",
    "        ('std', StandardScaler(), std_ftrs)])\n",
    "\n",
    "prep = Pipeline(steps=[('preprocessor', preprocessor)]) # for now we only preprocess, later we will add other steps here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz\n",
    "Let's recap preprocessing. Which of these statements are true?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class ParameterGrid in module sklearn.model_selection._search:\n",
      "\n",
      "class ParameterGrid(builtins.object)\n",
      " |  ParameterGrid(param_grid)\n",
      " |  \n",
      " |  Grid of parameters with a discrete number of values for each.\n",
      " |  \n",
      " |  Can be used to iterate over parameter value combinations with the\n",
      " |  Python built-in function iter.\n",
      " |  The order of the generated parameter combinations is deterministic.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <grid_search>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  param_grid : dict of str to sequence, or sequence of such\n",
      " |      The parameter grid to explore, as a dictionary mapping estimator\n",
      " |      parameters to sequences of allowed values.\n",
      " |  \n",
      " |      An empty dict signifies default parameters.\n",
      " |  \n",
      " |      A sequence of dicts signifies a sequence of grids to search, and is\n",
      " |      useful to avoid exploring parameter combinations that make no sense\n",
      " |      or have no effect. See the examples below.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.model_selection import ParameterGrid\n",
      " |  >>> param_grid = {'a': [1, 2], 'b': [True, False]}\n",
      " |  >>> list(ParameterGrid(param_grid)) == (\n",
      " |  ...    [{'a': 1, 'b': True}, {'a': 1, 'b': False},\n",
      " |  ...     {'a': 2, 'b': True}, {'a': 2, 'b': False}])\n",
      " |  True\n",
      " |  \n",
      " |  >>> grid = [{'kernel': ['linear']}, {'kernel': ['rbf'], 'gamma': [1, 10]}]\n",
      " |  >>> list(ParameterGrid(grid)) == [{'kernel': 'linear'},\n",
      " |  ...                               {'kernel': 'rbf', 'gamma': 1},\n",
      " |  ...                               {'kernel': 'rbf', 'gamma': 10}]\n",
      " |  True\n",
      " |  >>> ParameterGrid(grid)[1] == {'kernel': 'rbf', 'gamma': 1}\n",
      " |  True\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  GridSearchCV : Uses :class:`ParameterGrid` to perform a full parallelized\n",
      " |      parameter search.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getitem__(self, ind)\n",
      " |      Get the parameters that would be ``ind``th in iteration\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      ind : int\n",
      " |          The iteration index\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict of str to any\n",
      " |          Equal to list(self)[ind]\n",
      " |  \n",
      " |  __init__(self, param_grid)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Iterate over the points in the grid.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : iterator over dict of str to any\n",
      " |          Yields dictionaries mapping each estimator parameter to one of its\n",
      " |          allowed values.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Number of points on the grid.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ParameterGrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "randoms state 1\n",
      "    {'max_features': 0.25, 'max_depth': 1}\n",
      "    0.7599815724815725 0.7581388206388207\n",
      "    {'max_features': 0.5, 'max_depth': 1}\n",
      "    0.7599815724815725 0.7581388206388207\n",
      "    {'max_features': 0.75, 'max_depth': 1}\n",
      "    0.7599815724815725 0.7581388206388207\n",
      "    {'max_features': 1.0, 'max_depth': 1}\n",
      "    0.7599815724815725 0.7581388206388207\n",
      "    {'max_features': 0.25, 'max_depth': 3}\n",
      "    0.8408579033579033 0.8413697788697788\n",
      "    {'max_features': 0.5, 'max_depth': 3}\n",
      "    0.8433149058149059 0.8465909090909091\n",
      "    {'max_features': 0.75, 'max_depth': 3}\n",
      "    0.842956592956593 0.8459766584766585\n",
      "    {'max_features': 1.0, 'max_depth': 3}\n",
      "    0.8421375921375921 0.8456695331695332\n",
      "    {'max_features': 0.25, 'max_depth': 10}\n",
      "    0.8746928746928747 0.8616400491400491\n",
      "    {'max_features': 0.5, 'max_depth': 10}\n",
      "    0.8763308763308764 0.8627149877149877\n",
      "    {'max_features': 0.75, 'max_depth': 10}\n",
      "    0.8761261261261262 0.8614864864864865\n",
      "    {'max_features': 1.0, 'max_depth': 10}\n",
      "    0.8761773136773137 0.8614864864864865\n",
      "    {'max_features': 0.25, 'max_depth': 30}\n",
      "    0.9780917280917281 0.8547297297297297\n",
      "    {'max_features': 0.5, 'max_depth': 30}\n",
      "    0.9797809172809173 0.8541154791154791\n",
      "    {'max_features': 0.75, 'max_depth': 30}\n",
      "    0.9807534807534808 0.850583538083538\n",
      "    {'max_features': 1.0, 'max_depth': 30}\n",
      "    0.9805487305487306 0.8495085995085995\n",
      "    {'max_features': 0.25, 'max_depth': 100}\n",
      "    0.9819819819819819 0.8521191646191646\n",
      "    {'max_features': 0.5, 'max_depth': 100}\n",
      "    0.9819819819819819 0.851044226044226\n",
      "    {'max_features': 0.75, 'max_depth': 100}\n",
      "    0.9819819819819819 0.8511977886977887\n",
      "    {'max_features': 1.0, 'max_depth': 100}\n",
      "    0.9819819819819819 0.8487407862407862\n",
      "best model parameters: {'max_features': 0.5, 'max_depth': 10}\n",
      "corresponding validation score: 0.8627149877149877\n",
      "test score: 0.8624289881774911\n",
      "randoms state 2\n",
      "    {'max_features': 0.25, 'max_depth': 1}\n",
      "    0.7588554463554463 0.7547604422604423\n",
      "    {'max_features': 0.5, 'max_depth': 1}\n",
      "    0.7904381654381655 0.788544226044226\n",
      "    {'max_features': 0.75, 'max_depth': 1}\n",
      "    0.7588554463554463 0.7547604422604423\n",
      "    {'max_features': 1.0, 'max_depth': 1}\n",
      "    0.7588554463554463 0.7547604422604423\n",
      "    {'max_features': 0.25, 'max_depth': 3}\n",
      "    0.8409602784602784 0.836916461916462\n",
      "    {'max_features': 0.5, 'max_depth': 3}\n",
      "    0.8458742833742834 0.8398341523341524\n",
      "    {'max_features': 0.75, 'max_depth': 3}\n",
      "    0.8447481572481572 0.839527027027027\n",
      "    {'max_features': 1.0, 'max_depth': 3}\n",
      "    0.8448505323505323 0.8396805896805897\n",
      "    {'max_features': 0.25, 'max_depth': 10}\n",
      "    0.8752047502047502 0.8567260442260443\n",
      "    {'max_features': 0.5, 'max_depth': 10}\n",
      "    0.8781224406224406 0.8602579852579852\n",
      "    {'max_features': 0.75, 'max_depth': 10}\n",
      "    0.8779176904176904 0.8616400491400491\n",
      "    {'max_features': 1.0, 'max_depth': 10}\n",
      "    0.8778153153153153 0.859490171990172\n",
      "    {'max_features': 0.25, 'max_depth': 30}\n",
      "    0.9798321048321048 0.8485872235872236\n",
      "    {'max_features': 0.5, 'max_depth': 30}\n",
      "    0.9816748566748567 0.8508906633906634\n",
      "    {'max_features': 0.75, 'max_depth': 30}\n",
      "    0.9817772317772318 0.8498157248157249\n",
      "    {'max_features': 1.0, 'max_depth': 30}\n",
      "    0.9816236691236692 0.8487407862407862\n",
      "    {'max_features': 0.25, 'max_depth': 100}\n",
      "    0.9830569205569205 0.8482800982800983\n",
      "    {'max_features': 0.5, 'max_depth': 100}\n",
      "    0.9830569205569205 0.8468980343980343\n",
      "    {'max_features': 0.75, 'max_depth': 100}\n",
      "    0.9830569205569205 0.847512285012285\n",
      "    {'max_features': 1.0, 'max_depth': 100}\n",
      "    0.983005733005733 0.8459766584766585\n",
      "best model parameters: {'max_features': 0.75, 'max_depth': 10}\n",
      "corresponding validation score: 0.8616400491400491\n",
      "test score: 0.8615077537233226\n",
      "randoms state 3\n",
      "    {'max_features': 0.25, 'max_depth': 1}\n",
      "    0.7600839475839476 0.7530712530712531\n",
      "    {'max_features': 0.5, 'max_depth': 1}\n",
      "    0.7705773955773956 0.7627457002457002\n",
      "    {'max_features': 0.75, 'max_depth': 1}\n",
      "    0.7600839475839476 0.7530712530712531\n",
      "    {'max_features': 1.0, 'max_depth': 1}\n",
      "    0.7600839475839476 0.7530712530712531\n",
      "    {'max_features': 0.25, 'max_depth': 3}\n",
      "    0.8442362817362817 0.8353808353808354\n",
      "    {'max_features': 0.5, 'max_depth': 3}\n",
      "    0.846027846027846 0.8379914004914005\n",
      "    {'max_features': 0.75, 'max_depth': 3}\n",
      "    0.8456183456183456 0.8372235872235873\n",
      "    {'max_features': 1.0, 'max_depth': 3}\n",
      "    0.8456183456183456 0.8372235872235873\n",
      "    {'max_features': 0.25, 'max_depth': 10}\n",
      "    0.8738738738738738 0.856418918918919\n",
      "    {'max_features': 0.5, 'max_depth': 10}\n",
      "    0.8778153153153153 0.8593366093366094\n",
      "    {'max_features': 0.75, 'max_depth': 10}\n",
      "    0.8767403767403767 0.859029484029484\n",
      "    {'max_features': 1.0, 'max_depth': 10}\n",
      "    0.8759213759213759 0.8588759213759214\n",
      "    {'max_features': 0.25, 'max_depth': 30}\n",
      "    0.9781941031941032 0.8556511056511057\n",
      "    {'max_features': 0.5, 'max_depth': 30}\n",
      "    0.9801392301392301 0.8541154791154791\n",
      "    {'max_features': 0.75, 'max_depth': 30}\n",
      "    0.9804463554463555 0.8539619164619164\n",
      "    {'max_features': 1.0, 'max_depth': 30}\n",
      "    0.9805999180999181 0.8507371007371007\n",
      "    {'max_features': 0.25, 'max_depth': 100}\n",
      "    0.9813677313677314 0.8542690417690417\n",
      "    {'max_features': 0.5, 'max_depth': 100}\n",
      "    0.9813677313677314 0.8516584766584766\n",
      "    {'max_features': 0.75, 'max_depth': 100}\n",
      "    0.9813165438165438 0.8507371007371007\n",
      "    {'max_features': 1.0, 'max_depth': 100}\n",
      "    0.9813677313677314 0.8482800982800983\n",
      "best model parameters: {'max_features': 0.5, 'max_depth': 10}\n",
      "corresponding validation score: 0.8593366093366094\n",
      "test score: 0.8635037617073545\n",
      "randoms state 4\n",
      "    {'max_features': 0.25, 'max_depth': 1}\n",
      "    0.7657145782145782 0.754914004914005\n",
      "    {'max_features': 0.5, 'max_depth': 1}\n",
      "    0.7657145782145782 0.754914004914005\n",
      "    {'max_features': 0.75, 'max_depth': 1}\n",
      "    0.7657145782145782 0.754914004914005\n",
      "    {'max_features': 1.0, 'max_depth': 1}\n",
      "    0.7657145782145782 0.754914004914005\n",
      "    {'max_features': 0.25, 'max_depth': 3}\n",
      "    0.8441850941850941 0.8347665847665847\n",
      "    {'max_features': 0.5, 'max_depth': 3}\n",
      "    0.8479217854217854 0.8356879606879607\n",
      "    {'max_features': 0.75, 'max_depth': 3}\n",
      "    0.846488533988534 0.8361486486486487\n",
      "    {'max_features': 1.0, 'max_depth': 3}\n",
      "    0.846488533988534 0.836455773955774\n",
      "    {'max_features': 0.25, 'max_depth': 10}\n",
      "    0.877968877968878 0.859490171990172\n",
      "    {'max_features': 0.5, 'max_depth': 10}\n",
      "    0.8811936936936937 0.8584152334152334\n",
      "    {'max_features': 0.75, 'max_depth': 10}\n",
      "    0.8822686322686323 0.8591830466830467\n",
      "    {'max_features': 1.0, 'max_depth': 10}\n",
      "    0.883087633087633 0.8582616707616708\n",
      "    {'max_features': 0.25, 'max_depth': 30}\n",
      "    0.9804463554463555 0.8531941031941032\n",
      "    {'max_features': 0.5, 'max_depth': 30}\n",
      "    0.9817260442260443 0.8495085995085995\n",
      "    {'max_features': 0.75, 'max_depth': 30}\n",
      "    0.9822891072891073 0.8499692874692875\n",
      "    {'max_features': 1.0, 'max_depth': 30}\n",
      "    0.9823402948402948 0.847051597051597\n",
      "    {'max_features': 0.25, 'max_depth': 100}\n",
      "    0.9829545454545454 0.8484336609336609\n",
      "    {'max_features': 0.5, 'max_depth': 100}\n",
      "    0.9829545454545454 0.8476658476658476\n",
      "    {'max_features': 0.75, 'max_depth': 100}\n",
      "    0.9829545454545454 0.8481265356265356\n",
      "    {'max_features': 1.0, 'max_depth': 100}\n",
      "    0.9829545454545454 0.8462837837837838\n",
      "best model parameters: {'max_features': 0.25, 'max_depth': 10}\n",
      "corresponding validation score: 0.859490171990172\n",
      "test score: 0.8582834331337326\n",
      "randoms state 5\n",
      "    {'max_features': 0.25, 'max_depth': 1}\n",
      "    0.756961506961507 0.7590601965601965\n",
      "    {'max_features': 0.5, 'max_depth': 1}\n",
      "    0.7872133497133497 0.7926904176904177\n",
      "    {'max_features': 0.75, 'max_depth': 1}\n",
      "    0.756961506961507 0.7590601965601965\n",
      "    {'max_features': 1.0, 'max_depth': 1}\n",
      "    0.756961506961507 0.7590601965601965\n",
      "    {'max_features': 0.25, 'max_depth': 3}\n",
      "    0.833947583947584 0.8381449631449631\n",
      "    {'max_features': 0.5, 'max_depth': 3}\n",
      "    0.842495904995905 0.8465909090909091\n",
      "    {'max_features': 0.75, 'max_depth': 3}\n",
      "    0.8420864045864046 0.8467444717444718\n",
      "    {'max_features': 1.0, 'max_depth': 3}\n",
      "    0.8420864045864046 0.8468980343980343\n",
      "    {'max_features': 0.25, 'max_depth': 10}\n",
      "    0.8734643734643734 0.8617936117936118\n",
      "    {'max_features': 0.5, 'max_depth': 10}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0.8764332514332515 0.8608722358722358\n",
      "    {'max_features': 0.75, 'max_depth': 10}\n",
      "    0.8766380016380017 0.860411547911548\n",
      "    {'max_features': 1.0, 'max_depth': 10}\n",
      "    0.8754095004095004 0.85995085995086\n",
      "    {'max_features': 0.25, 'max_depth': 30}\n",
      "    0.9787571662571662 0.8548832923832924\n",
      "    {'max_features': 0.5, 'max_depth': 30}\n",
      "    0.980497542997543 0.8527334152334153\n",
      "    {'max_features': 0.75, 'max_depth': 30}\n",
      "    0.9809070434070434 0.8495085995085995\n",
      "    {'max_features': 1.0, 'max_depth': 30}\n",
      "    0.9808046683046683 0.8482800982800983\n",
      "    {'max_features': 0.25, 'max_depth': 100}\n",
      "    0.9816748566748567 0.8487407862407862\n",
      "    {'max_features': 0.5, 'max_depth': 100}\n",
      "    0.9816748566748567 0.8502764127764127\n",
      "    {'max_features': 0.75, 'max_depth': 100}\n",
      "    0.9816748566748567 0.8490479115479116\n",
      "    {'max_features': 1.0, 'max_depth': 100}\n",
      "    0.9816236691236692 0.847972972972973\n",
      "best model parameters: {'max_features': 0.25, 'max_depth': 10}\n",
      "corresponding validation score: 0.8617936117936118\n",
      "test score: 0.8641179180101336\n"
     ]
    }
   ],
   "source": [
    "# let's train a random forest classifier\n",
    "\n",
    "# we will loop through nr_states random states so we will return nr_states test scores and nr_states trained models\n",
    "nr_states = 5\n",
    "test_scores = np.zeros(nr_states)\n",
    "final_models = []\n",
    "\n",
    "# loop through the different random states\n",
    "for i in range(nr_states):\n",
    "    print('randoms state '+str(i+1))\n",
    "\n",
    "    # first split to separate out the training set\n",
    "    X_train, X_other, y_train, y_other = train_test_split(X,y,train_size = 0.6,random_state=42*i)\n",
    "\n",
    "    # second split to separate out the validation and test sets\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_other,y_other,train_size = 0.5,random_state=42*i)\n",
    "    \n",
    "    # preprocess the sets\n",
    "    X_train_prep = prep.fit_transform(X_train)\n",
    "    X_val_prep = prep.transform(X_val)\n",
    "    X_test_prep = prep.transform(X_test)\n",
    "\n",
    "    # decide which parameters to tune and what values to try\n",
    "    # the default value of any parameter not specified here will be used\n",
    "    param_grid = {\n",
    "                  'max_depth': [1, 3, 10, 30, 100], # no upper bound so the values are evenly spaced in log\n",
    "                  'max_features': [0.25, 0.5,0.75,1.0] # linearly spaced because it is between 0 and 1, 0 is omitted\n",
    "                  } \n",
    "\n",
    "    # we save the train and validation scores\n",
    "    # the validation scores are necessary to select the best model\n",
    "    # it's optional to save the train scores, it can be used to identify high bias and high variance models\n",
    "    train_score = np.zeros(len(ParameterGrid(param_grid)))\n",
    "    val_score = np.zeros(len(ParameterGrid(param_grid)))\n",
    "    models = []\n",
    "    \n",
    "    # loop through all combinations of hyperparameter combos\n",
    "    for p in range(len(ParameterGrid(param_grid))):\n",
    "        params = ParameterGrid(param_grid)[p]\n",
    "        print('   ',params) \n",
    "        clf = RandomForestClassifier(**params,random_state = 42*i,n_jobs=-1) # initialize the classifier\n",
    "        clf.fit(X_train_prep,y_train) # fit the model\n",
    "        models.append(clf) # save it\n",
    "        # calculate train and validation accuracy scores\n",
    "        y_train_pred = clf.predict(X_train_prep)\n",
    "        train_score[p] = accuracy_score(y_train,y_train_pred)\n",
    "        y_val_pred = clf.predict(X_val_prep)\n",
    "        val_score[p] = accuracy_score(y_val,y_val_pred)\n",
    "        print('   ',train_score[p],val_score[p])\n",
    "    \n",
    "    # print out model parameters that maximize validation accuracy\n",
    "    print('best model parameters:',ParameterGrid(param_grid)[np.argmax(val_score)])\n",
    "    print('corresponding validation score:',np.max(val_score))\n",
    "    # collect and save the best model\n",
    "    final_models.append(models[np.argmax(val_score)])\n",
    "    # calculate and save the test score\n",
    "    y_test_pred = final_models[-1].predict(X_test_prep)\n",
    "    test_scores[i] = accuracy_score(y_test,y_test_pred)\n",
    "    print('test score:',test_scores[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things to look out for\n",
    "- are the ranges of the hyperparameters wide enough?\n",
    "    - if you are unsure, save the training scores and plot the train and val scores!\n",
    "    - do you see underfitting? model performs poorly on both training and validation sets?\n",
    "    - do you see overfitting? model performs very good on training but worse on validation?\n",
    "    - if you don't see both, expand the range of the parameters and you'll likely find a better model\n",
    "    - read the manual and make sure you understand what the hyperparameter does in the model\n",
    "        - some parameters (like regularization parameters) should be evenly spaced in log because there is no upper bound\n",
    "        - some parameters (like max_features) should be linearly spaced because they have clear lower and upper bounds\n",
    "    - if the best hyperparameter is at the edge of your range, you definitely need to expand the range if you can\n",
    "- not every hyperparameter is equally important\n",
    "    - some parameters have little to no impact on train and validation scores\n",
    "    - in the example above, max_depth is much more important than max_features\n",
    "    - visualize the results if in doubt\n",
    "- is the best validation score similar to the test score?\n",
    "    - it's usual that the validation score is a bit better than the test score\n",
    "    - but if the difference between the two scores is significant over multiple random states, something could be off\n",
    "- traiv/val/test split is usually a safe bet for any splitting strategy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning with folds\n",
    "- the steps are a bit different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "df = pd.read_csv('data/adult_data.csv')\n",
    "\n",
    "# let's separate the feature matrix X, and target variable y\n",
    "y = df['gross-income'] # remember, we want to predict who earns more than 50k or less than 50k\n",
    "X = df.loc[:, df.columns != 'gross-income'] # all other columns are features\n",
    "\n",
    "ordinal_ftrs = ['education'] \n",
    "ordinal_cats = [[' Preschool',' 1st-4th',' 5th-6th',' 7th-8th',' 9th',' 10th',' 11th',' 12th',' HS-grad',\\\n",
    "                ' Some-college',' Assoc-voc',' Assoc-acdm',' Bachelors',' Masters',' Prof-school',' Doctorate']]\n",
    "onehot_ftrs = ['workclass','marital-status','occupation','relationship','race','sex','native-country']\n",
    "minmax_ftrs = ['age','hours-per-week']\n",
    "std_ftrs = ['capital-gain','capital-loss']\n",
    "\n",
    "# collect all the encoders\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('ord', OrdinalEncoder(categories = ordinal_cats), ordinal_ftrs),\n",
    "        ('onehot', OneHotEncoder(sparse=False,handle_unknown='ignore'), onehot_ftrs),\n",
    "        ('minmax', MinMaxScaler(), minmax_ftrs),\n",
    "        ('std', StandardScaler(), std_ftrs)])\n",
    "\n",
    "# all the same up to this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 15 candidates, totalling 60 fits\n",
      "best model parameters: {'randomforestclassifier__max_depth': 10, 'randomforestclassifier__max_features': 0.75}\n",
      "validation score: 0.8628685503685503\n",
      "test score: 0.8576692768309535\n",
      "Fitting 4 folds for each of 15 candidates, totalling 60 fits\n",
      "best model parameters: {'randomforestclassifier__max_depth': 10, 'randomforestclassifier__max_features': 0.75}\n",
      "validation score: 0.8601428132678133\n",
      "test score: 0.865806847842776\n",
      "Fitting 4 folds for each of 15 candidates, totalling 60 fits\n",
      "best model parameters: {'randomforestclassifier__max_depth': 10, 'randomforestclassifier__max_features': 0.5}\n",
      "validation score: 0.8624846437346437\n",
      "test score: 0.8590511285122063\n"
     ]
    }
   ],
   "source": [
    "# we will use GridSearchCV and the parameter names need to contain the ML algorithm you want to use\n",
    "# the parameters of some ML algorithms have the same name and this is how we avoid confusion\n",
    "param_grid = {\n",
    "              'randomforestclassifier__max_depth': [1, 3, 10, 30, 100], # the max_depth should be smaller or equal than the number of features roughly\n",
    "              'randomforestclassifier__max_features': [0.5,0.75,1.0] # linearly spaced between 0.5 and 1\n",
    "              } \n",
    "\n",
    "nr_states = 3\n",
    "test_scores = np.zeros(nr_states)\n",
    "final_models = []\n",
    "\n",
    "for i in range(nr_states):\n",
    "    # first split to separate out the test set\n",
    "    # we will use kfold on other\n",
    "    X_other, X_test, y_other, y_test = train_test_split(X,y,test_size = 0.2,random_state=42*i)\n",
    "\n",
    "    # splitter for other\n",
    "    kf = KFold(n_splits=4,shuffle=True,random_state=42*i)\n",
    "\n",
    "    # the classifier\n",
    "    clf = RandomForestClassifier(random_state = 42*i) # initialize the classifier\n",
    "\n",
    "    # let's put together a pipeline\n",
    "    # the pipeline will fit_transform the training set (3 folds), and transform the last fold used as validation\n",
    "    # then it will train the ML algorithm on the training set and evaluate it on the validation set\n",
    "    # it repeats this step automatically such that each fold will be an evaluation set once\n",
    "    pipe = make_pipeline(preprocessor,clf)\n",
    "\n",
    "    # use GridSearchCV\n",
    "    # GridSearchCV loops through all parameter combinations and collects the results \n",
    "    grid = GridSearchCV(pipe, param_grid=param_grid,scoring = 'accuracy',\n",
    "                        cv=kf, return_train_score = True, n_jobs=-1, verbose=True)\n",
    "    \n",
    "    # this line actually fits the model on other\n",
    "    grid.fit(X_other, y_other)\n",
    "    # save results into a data frame. feel free to print it and inspect it\n",
    "    results = pd.DataFrame(grid.cv_results_)\n",
    "    #print(results)\n",
    "\n",
    "    print('best model parameters:',grid.best_params_)\n",
    "    print('validation score:',grid.best_score_) # this is the mean validation score over all iterations\n",
    "    # save the model\n",
    "    final_models.append(grid)\n",
    "    # calculate and save the test score\n",
    "    y_test_pred = final_models[-1].predict(X_test)\n",
    "    test_scores[i] = accuracy_score(y_test,y_test_pred)\n",
    "    print('test score:',test_scores[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_randomforestclassifier__max_depth</th>\n",
       "      <th>param_randomforestclassifier__max_features</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.181422</td>\n",
       "      <td>0.038313</td>\n",
       "      <td>0.205140</td>\n",
       "      <td>0.020082</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'randomforestclassifier__max_depth': 1, 'rand...</td>\n",
       "      <td>0.772881</td>\n",
       "      <td>0.787469</td>\n",
       "      <td>0.763514</td>\n",
       "      <td>0.792690</td>\n",
       "      <td>0.779139</td>\n",
       "      <td>0.011580</td>\n",
       "      <td>13</td>\n",
       "      <td>0.780405</td>\n",
       "      <td>0.793561</td>\n",
       "      <td>0.757729</td>\n",
       "      <td>0.786701</td>\n",
       "      <td>0.779599</td>\n",
       "      <td>0.013456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.758087</td>\n",
       "      <td>0.027262</td>\n",
       "      <td>0.151116</td>\n",
       "      <td>0.013382</td>\n",
       "      <td>1</td>\n",
       "      <td>0.75</td>\n",
       "      <td>{'randomforestclassifier__max_depth': 1, 'rand...</td>\n",
       "      <td>0.754300</td>\n",
       "      <td>0.754607</td>\n",
       "      <td>0.763514</td>\n",
       "      <td>0.793305</td>\n",
       "      <td>0.766431</td>\n",
       "      <td>0.015951</td>\n",
       "      <td>14</td>\n",
       "      <td>0.760801</td>\n",
       "      <td>0.760698</td>\n",
       "      <td>0.757729</td>\n",
       "      <td>0.787469</td>\n",
       "      <td>0.766674</td>\n",
       "      <td>0.012069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.350122</td>\n",
       "      <td>0.079547</td>\n",
       "      <td>0.196142</td>\n",
       "      <td>0.014781</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'randomforestclassifier__max_depth': 1, 'rand...</td>\n",
       "      <td>0.754300</td>\n",
       "      <td>0.754607</td>\n",
       "      <td>0.763514</td>\n",
       "      <td>0.764281</td>\n",
       "      <td>0.759175</td>\n",
       "      <td>0.004731</td>\n",
       "      <td>15</td>\n",
       "      <td>0.760801</td>\n",
       "      <td>0.760698</td>\n",
       "      <td>0.757729</td>\n",
       "      <td>0.757473</td>\n",
       "      <td>0.759175</td>\n",
       "      <td>0.001577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.463053</td>\n",
       "      <td>0.027966</td>\n",
       "      <td>0.217251</td>\n",
       "      <td>0.015049</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'randomforestclassifier__max_depth': 3, 'rand...</td>\n",
       "      <td>0.838299</td>\n",
       "      <td>0.843366</td>\n",
       "      <td>0.847973</td>\n",
       "      <td>0.851505</td>\n",
       "      <td>0.845286</td>\n",
       "      <td>0.004960</td>\n",
       "      <td>10</td>\n",
       "      <td>0.847871</td>\n",
       "      <td>0.846233</td>\n",
       "      <td>0.844390</td>\n",
       "      <td>0.843776</td>\n",
       "      <td>0.845567</td>\n",
       "      <td>0.001608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.143098</td>\n",
       "      <td>0.089503</td>\n",
       "      <td>0.237941</td>\n",
       "      <td>0.015617</td>\n",
       "      <td>3</td>\n",
       "      <td>0.75</td>\n",
       "      <td>{'randomforestclassifier__max_depth': 3, 'rand...</td>\n",
       "      <td>0.837684</td>\n",
       "      <td>0.841984</td>\n",
       "      <td>0.847205</td>\n",
       "      <td>0.850891</td>\n",
       "      <td>0.844441</td>\n",
       "      <td>0.005023</td>\n",
       "      <td>12</td>\n",
       "      <td>0.846847</td>\n",
       "      <td>0.845311</td>\n",
       "      <td>0.843622</td>\n",
       "      <td>0.842496</td>\n",
       "      <td>0.844569</td>\n",
       "      <td>0.001653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7.667399</td>\n",
       "      <td>0.059537</td>\n",
       "      <td>0.226158</td>\n",
       "      <td>0.010623</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'randomforestclassifier__max_depth': 3, 'rand...</td>\n",
       "      <td>0.837684</td>\n",
       "      <td>0.842138</td>\n",
       "      <td>0.847359</td>\n",
       "      <td>0.851044</td>\n",
       "      <td>0.844556</td>\n",
       "      <td>0.005075</td>\n",
       "      <td>11</td>\n",
       "      <td>0.846847</td>\n",
       "      <td>0.845362</td>\n",
       "      <td>0.843622</td>\n",
       "      <td>0.842394</td>\n",
       "      <td>0.844556</td>\n",
       "      <td>0.001692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9.119380</td>\n",
       "      <td>0.099135</td>\n",
       "      <td>0.228989</td>\n",
       "      <td>0.010201</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'randomforestclassifier__max_depth': 10, 'ran...</td>\n",
       "      <td>0.857647</td>\n",
       "      <td>0.859644</td>\n",
       "      <td>0.865479</td>\n",
       "      <td>0.867168</td>\n",
       "      <td>0.862485</td>\n",
       "      <td>0.003949</td>\n",
       "      <td>1</td>\n",
       "      <td>0.880170</td>\n",
       "      <td>0.880989</td>\n",
       "      <td>0.878225</td>\n",
       "      <td>0.875819</td>\n",
       "      <td>0.878801</td>\n",
       "      <td>0.001993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12.822704</td>\n",
       "      <td>0.115817</td>\n",
       "      <td>0.214337</td>\n",
       "      <td>0.013214</td>\n",
       "      <td>10</td>\n",
       "      <td>0.75</td>\n",
       "      <td>{'randomforestclassifier__max_depth': 10, 'ran...</td>\n",
       "      <td>0.857801</td>\n",
       "      <td>0.858876</td>\n",
       "      <td>0.864711</td>\n",
       "      <td>0.868090</td>\n",
       "      <td>0.862369</td>\n",
       "      <td>0.004221</td>\n",
       "      <td>2</td>\n",
       "      <td>0.880477</td>\n",
       "      <td>0.881347</td>\n",
       "      <td>0.879453</td>\n",
       "      <td>0.876024</td>\n",
       "      <td>0.879325</td>\n",
       "      <td>0.002021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>16.088822</td>\n",
       "      <td>0.071157</td>\n",
       "      <td>0.234474</td>\n",
       "      <td>0.013608</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'randomforestclassifier__max_depth': 10, 'ran...</td>\n",
       "      <td>0.857033</td>\n",
       "      <td>0.858569</td>\n",
       "      <td>0.863329</td>\n",
       "      <td>0.868704</td>\n",
       "      <td>0.861909</td>\n",
       "      <td>0.004558</td>\n",
       "      <td>3</td>\n",
       "      <td>0.879863</td>\n",
       "      <td>0.881245</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.874898</td>\n",
       "      <td>0.878698</td>\n",
       "      <td>0.002361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11.278563</td>\n",
       "      <td>0.125260</td>\n",
       "      <td>0.248746</td>\n",
       "      <td>0.019503</td>\n",
       "      <td>30</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'randomforestclassifier__max_depth': 30, 'ran...</td>\n",
       "      <td>0.850276</td>\n",
       "      <td>0.852273</td>\n",
       "      <td>0.848434</td>\n",
       "      <td>0.863329</td>\n",
       "      <td>0.853578</td>\n",
       "      <td>0.005791</td>\n",
       "      <td>4</td>\n",
       "      <td>0.980446</td>\n",
       "      <td>0.979986</td>\n",
       "      <td>0.980498</td>\n",
       "      <td>0.980242</td>\n",
       "      <td>0.980293</td>\n",
       "      <td>0.000202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>15.842812</td>\n",
       "      <td>0.263227</td>\n",
       "      <td>0.242607</td>\n",
       "      <td>0.019124</td>\n",
       "      <td>30</td>\n",
       "      <td>0.75</td>\n",
       "      <td>{'randomforestclassifier__max_depth': 30, 'ran...</td>\n",
       "      <td>0.849509</td>\n",
       "      <td>0.849662</td>\n",
       "      <td>0.849048</td>\n",
       "      <td>0.857955</td>\n",
       "      <td>0.851543</td>\n",
       "      <td>0.003708</td>\n",
       "      <td>6</td>\n",
       "      <td>0.980907</td>\n",
       "      <td>0.980242</td>\n",
       "      <td>0.980805</td>\n",
       "      <td>0.980395</td>\n",
       "      <td>0.980587</td>\n",
       "      <td>0.000277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>20.856934</td>\n",
       "      <td>0.608109</td>\n",
       "      <td>0.335563</td>\n",
       "      <td>0.025797</td>\n",
       "      <td>30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'randomforestclassifier__max_depth': 30, 'ran...</td>\n",
       "      <td>0.847819</td>\n",
       "      <td>0.850737</td>\n",
       "      <td>0.846437</td>\n",
       "      <td>0.857801</td>\n",
       "      <td>0.850699</td>\n",
       "      <td>0.004384</td>\n",
       "      <td>7</td>\n",
       "      <td>0.981112</td>\n",
       "      <td>0.980293</td>\n",
       "      <td>0.980753</td>\n",
       "      <td>0.980446</td>\n",
       "      <td>0.980651</td>\n",
       "      <td>0.000313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11.742924</td>\n",
       "      <td>0.217466</td>\n",
       "      <td>0.289400</td>\n",
       "      <td>0.018626</td>\n",
       "      <td>100</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'randomforestclassifier__max_depth': 100, 'ra...</td>\n",
       "      <td>0.848741</td>\n",
       "      <td>0.850430</td>\n",
       "      <td>0.847666</td>\n",
       "      <td>0.860104</td>\n",
       "      <td>0.851735</td>\n",
       "      <td>0.004931</td>\n",
       "      <td>5</td>\n",
       "      <td>0.982136</td>\n",
       "      <td>0.981061</td>\n",
       "      <td>0.981828</td>\n",
       "      <td>0.981214</td>\n",
       "      <td>0.981560</td>\n",
       "      <td>0.000439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>16.258868</td>\n",
       "      <td>0.193122</td>\n",
       "      <td>0.244635</td>\n",
       "      <td>0.020889</td>\n",
       "      <td>100</td>\n",
       "      <td>0.75</td>\n",
       "      <td>{'randomforestclassifier__max_depth': 100, 'ra...</td>\n",
       "      <td>0.847973</td>\n",
       "      <td>0.847973</td>\n",
       "      <td>0.843827</td>\n",
       "      <td>0.856265</td>\n",
       "      <td>0.849010</td>\n",
       "      <td>0.004518</td>\n",
       "      <td>8</td>\n",
       "      <td>0.982136</td>\n",
       "      <td>0.981061</td>\n",
       "      <td>0.981828</td>\n",
       "      <td>0.981214</td>\n",
       "      <td>0.981560</td>\n",
       "      <td>0.000439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>16.597886</td>\n",
       "      <td>0.199887</td>\n",
       "      <td>0.145838</td>\n",
       "      <td>0.015063</td>\n",
       "      <td>100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'randomforestclassifier__max_depth': 100, 'ra...</td>\n",
       "      <td>0.845516</td>\n",
       "      <td>0.848741</td>\n",
       "      <td>0.844441</td>\n",
       "      <td>0.856419</td>\n",
       "      <td>0.848779</td>\n",
       "      <td>0.004686</td>\n",
       "      <td>9</td>\n",
       "      <td>0.982084</td>\n",
       "      <td>0.980958</td>\n",
       "      <td>0.981828</td>\n",
       "      <td>0.981214</td>\n",
       "      <td>0.981521</td>\n",
       "      <td>0.000454</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        2.181422      0.038313         0.205140        0.020082   \n",
       "1        2.758087      0.027262         0.151116        0.013382   \n",
       "2        3.350122      0.079547         0.196142        0.014781   \n",
       "3        4.463053      0.027966         0.217251        0.015049   \n",
       "4        6.143098      0.089503         0.237941        0.015617   \n",
       "5        7.667399      0.059537         0.226158        0.010623   \n",
       "6        9.119380      0.099135         0.228989        0.010201   \n",
       "7       12.822704      0.115817         0.214337        0.013214   \n",
       "8       16.088822      0.071157         0.234474        0.013608   \n",
       "9       11.278563      0.125260         0.248746        0.019503   \n",
       "10      15.842812      0.263227         0.242607        0.019124   \n",
       "11      20.856934      0.608109         0.335563        0.025797   \n",
       "12      11.742924      0.217466         0.289400        0.018626   \n",
       "13      16.258868      0.193122         0.244635        0.020889   \n",
       "14      16.597886      0.199887         0.145838        0.015063   \n",
       "\n",
       "   param_randomforestclassifier__max_depth  \\\n",
       "0                                        1   \n",
       "1                                        1   \n",
       "2                                        1   \n",
       "3                                        3   \n",
       "4                                        3   \n",
       "5                                        3   \n",
       "6                                       10   \n",
       "7                                       10   \n",
       "8                                       10   \n",
       "9                                       30   \n",
       "10                                      30   \n",
       "11                                      30   \n",
       "12                                     100   \n",
       "13                                     100   \n",
       "14                                     100   \n",
       "\n",
       "   param_randomforestclassifier__max_features  \\\n",
       "0                                         0.5   \n",
       "1                                        0.75   \n",
       "2                                         1.0   \n",
       "3                                         0.5   \n",
       "4                                        0.75   \n",
       "5                                         1.0   \n",
       "6                                         0.5   \n",
       "7                                        0.75   \n",
       "8                                         1.0   \n",
       "9                                         0.5   \n",
       "10                                       0.75   \n",
       "11                                        1.0   \n",
       "12                                        0.5   \n",
       "13                                       0.75   \n",
       "14                                        1.0   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "0   {'randomforestclassifier__max_depth': 1, 'rand...           0.772881   \n",
       "1   {'randomforestclassifier__max_depth': 1, 'rand...           0.754300   \n",
       "2   {'randomforestclassifier__max_depth': 1, 'rand...           0.754300   \n",
       "3   {'randomforestclassifier__max_depth': 3, 'rand...           0.838299   \n",
       "4   {'randomforestclassifier__max_depth': 3, 'rand...           0.837684   \n",
       "5   {'randomforestclassifier__max_depth': 3, 'rand...           0.837684   \n",
       "6   {'randomforestclassifier__max_depth': 10, 'ran...           0.857647   \n",
       "7   {'randomforestclassifier__max_depth': 10, 'ran...           0.857801   \n",
       "8   {'randomforestclassifier__max_depth': 10, 'ran...           0.857033   \n",
       "9   {'randomforestclassifier__max_depth': 30, 'ran...           0.850276   \n",
       "10  {'randomforestclassifier__max_depth': 30, 'ran...           0.849509   \n",
       "11  {'randomforestclassifier__max_depth': 30, 'ran...           0.847819   \n",
       "12  {'randomforestclassifier__max_depth': 100, 'ra...           0.848741   \n",
       "13  {'randomforestclassifier__max_depth': 100, 'ra...           0.847973   \n",
       "14  {'randomforestclassifier__max_depth': 100, 'ra...           0.845516   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  mean_test_score  \\\n",
       "0            0.787469           0.763514           0.792690         0.779139   \n",
       "1            0.754607           0.763514           0.793305         0.766431   \n",
       "2            0.754607           0.763514           0.764281         0.759175   \n",
       "3            0.843366           0.847973           0.851505         0.845286   \n",
       "4            0.841984           0.847205           0.850891         0.844441   \n",
       "5            0.842138           0.847359           0.851044         0.844556   \n",
       "6            0.859644           0.865479           0.867168         0.862485   \n",
       "7            0.858876           0.864711           0.868090         0.862369   \n",
       "8            0.858569           0.863329           0.868704         0.861909   \n",
       "9            0.852273           0.848434           0.863329         0.853578   \n",
       "10           0.849662           0.849048           0.857955         0.851543   \n",
       "11           0.850737           0.846437           0.857801         0.850699   \n",
       "12           0.850430           0.847666           0.860104         0.851735   \n",
       "13           0.847973           0.843827           0.856265         0.849010   \n",
       "14           0.848741           0.844441           0.856419         0.848779   \n",
       "\n",
       "    std_test_score  rank_test_score  split0_train_score  split1_train_score  \\\n",
       "0         0.011580               13            0.780405            0.793561   \n",
       "1         0.015951               14            0.760801            0.760698   \n",
       "2         0.004731               15            0.760801            0.760698   \n",
       "3         0.004960               10            0.847871            0.846233   \n",
       "4         0.005023               12            0.846847            0.845311   \n",
       "5         0.005075               11            0.846847            0.845362   \n",
       "6         0.003949                1            0.880170            0.880989   \n",
       "7         0.004221                2            0.880477            0.881347   \n",
       "8         0.004558                3            0.879863            0.881245   \n",
       "9         0.005791                4            0.980446            0.979986   \n",
       "10        0.003708                6            0.980907            0.980242   \n",
       "11        0.004384                7            0.981112            0.980293   \n",
       "12        0.004931                5            0.982136            0.981061   \n",
       "13        0.004518                8            0.982136            0.981061   \n",
       "14        0.004686                9            0.982084            0.980958   \n",
       "\n",
       "    split2_train_score  split3_train_score  mean_train_score  std_train_score  \n",
       "0             0.757729            0.786701          0.779599         0.013456  \n",
       "1             0.757729            0.787469          0.766674         0.012069  \n",
       "2             0.757729            0.757473          0.759175         0.001577  \n",
       "3             0.844390            0.843776          0.845567         0.001608  \n",
       "4             0.843622            0.842496          0.844569         0.001653  \n",
       "5             0.843622            0.842394          0.844556         0.001692  \n",
       "6             0.878225            0.875819          0.878801         0.001993  \n",
       "7             0.879453            0.876024          0.879325         0.002021  \n",
       "8             0.878788            0.874898          0.878698         0.002361  \n",
       "9             0.980498            0.980242          0.980293         0.000202  \n",
       "10            0.980805            0.980395          0.980587         0.000277  \n",
       "11            0.980753            0.980446          0.980651         0.000313  \n",
       "12            0.981828            0.981214          0.981560         0.000439  \n",
       "13            0.981828            0.981214          0.981560         0.000439  \n",
       "14            0.981828            0.981214          0.981521         0.000454  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things to look out for\n",
    "- less code but more stuff is going on in the background hidden from you\n",
    "    - looping over multiple folds\n",
    "    - .fit_transform and .transform is hidden from you\n",
    "- nevertheless, GridSearchCV and pipelines are pretty powerful\n",
    "- working with folds is a bit more robust because the best hyperparameter is selected based on the average score of multiple trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz\n",
    "Can we use GridSearchCV with sets prepared by train_test_split in advance? Use the sklearn manual or stackoverflow to answer the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mud card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
