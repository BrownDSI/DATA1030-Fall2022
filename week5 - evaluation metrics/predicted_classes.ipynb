{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mudcard\n",
    "- **should you ever remove a feature ? Or will it always be beneficial. I like to drop columns where 80% of the values are empty.**\n",
    "    - I'd only drop unique IDs like the ID column in the house price dataset\n",
    "    - I wouldn't drop a feature if 80% of values are empty\n",
    "    - that remaining 20% could be helpful or the missingness might correlated with the target variable somehow\n",
    "        - e.g. in classification, 90% of missing values are in class 0, and 10% are in class 1\n",
    "        - e.g., in regression, the probability of a value missing correlates with the target variable (if the target variable is large, it is more likely the feature will be missing)\n",
    "        - some ML techniques can learn these sort of things to improve predictive power\n",
    "- **For continuous feature, can we use random number (which follows the distribution of existing values) to fill the missing values?**\n",
    "    - I wouldn't \n",
    "    - let's assume the feature is some sort of test result which is performed on some patients but it is unnecessary for other patiens like measuring the concentration of certain platelets\n",
    "    - would you feel comfortable randomly guessing what the platelet count is for patients without test?\n",
    "- **Can you preform feature engineering on your target variable in order to create a new category for prediction?**\n",
    "    - NO!!!\n",
    "    - absolutely not\n",
    "    - when you deploy the model and it is time to use it on previously unseen data, how would you determine that new category without knowing the target variable?\n",
    "    - this would be a very bad case of information leakage\n",
    "- **What is the difference between the feature selection approaches we covered at the end of class and using 'Stepwise regression' to identify informative features? Is the latter less flexible or useful in some cases?**\n",
    "    - stepwise regression/classification is more powerful because you use an ML model to select features so feature interactions can be taken into account\n",
    "    - the f test and the mutual info calculates how strongly each feature correlates with the target variable but it ignores feature interactions\n",
    "    - we will cover a couple of techniques to select features using ML models like permutation feature importance, shap, Lasso or l1 regularization\n",
    "- **\"I am confused as to why the polynomialFeatures is useful, is it random added data?**\n",
    "- **I understand why creating a feature like a x b is useful for capturing feature interactions, but what is the point of polynomial features like a^2, b^2, if already have a and b.**\n",
    "    - if you use a linear model and sqrt(a) + noise = y, model performance will improve if you add a^2 because now a^2 and y will be linearly correlated\n",
    "    - these are very handwavy arguments\n",
    "    - as I said in class, automated feature engineering is rarely useful\n",
    "- **Conceptually, the idea of transformations doesn't seem to add any new information to me but maybe that's because I don't understand how the algorithms fail to make those calculations on their own.**\n",
    "    - you are correct, transformations don't add new info\n",
    "    - it will help the ML model to converge faster though\n",
    "    - read more [here](https://towardsdatascience.com/all-about-feature-scaling-bcc0ad75cb35)\n",
    "- **Near the beginning of lecture when we talked about transforming the categorical, nominal, and numeric columns, I am confused on how we know what preprocessor.fit_transform and preprocessor.get_feature_names_out do**\n",
    "    - check their manuals\n",
    "- **If there is a relationship between more than 2 features, how will we find out because it is difficult to visualize data in higher dimensions.**\n",
    "    - yes, that's difficult\n",
    "    - you can maybe visualize three features in 3D but it's tough to go beyond that\n",
    "- **will adding more features ever make the model worse?**\n",
    "    - good question!\n",
    "    - depends on the model\n",
    "    - tree-based methods will not get worse if you add uninformative features but k nearest neightbors will\n",
    "    - once we cover ML algorithms, you could create a toy dataset, add uninformative features and study this effect\n",
    "- **If the feature selection that we covered today happens before data splitting, why are you teaching it after data splitting? I guess it is because it is a relatively not-important topic?**\n",
    "    - yeah\n",
    "    - automated feature engineering and feature selection without an ML model are short topics you might or might not need in the future\n",
    "    - it doesn't hurt to at least hear about them but they are not really all that useful in practice\n",
    "- **if the degree were to increase, is it just like multiplying everything out as if it were algebra? so three integers its a*a*a but two degrees is just a*a and would there still be a*a in 3 degrees?**\n",
    "    - run the code and check it yourself! :)\n",
    "- **Do time series data require feature engineering?**\n",
    "    - yes, autoregression is basically feature engineering for time series data\n",
    "- **Could more advanced statistical methods be applied to the dataset and then added as a new feature? Or should we stick to basic calculations for the new features?**\n",
    "    - you can create any feature you'd like\n",
    "- **As mentioned in class, we are doing feature engineering after we choose a model. How do we know what is the best feature-model combo? What if the feature we engineered can achieve better performance on other models?**\n",
    "    - always check the generalization error, the test score\n",
    "    - it is a good idea to engineer one feature and run it through multiple models and check which model is the best\n",
    "- **I understand the value of adding features and not preemptively taking them out before modelling. However, doesn't adding too many features also run the risk of overfitting and making the model less generalizable to datasets that aren't coded exactly the same way?**\n",
    "    - overfitting is avoided with cross-validation and measuring the bias-variance trade-off\n",
    "    - classical supervised ML models almost never generalize to other datasets/projects\n",
    "    - your model will be specific to the dataset you trained it on\n",
    "- **Would feature engineering also entail filling in missing values based on other datasets. For example, if we had weather data, could we use the weather from the year prior to help fill in values?**\n",
    "    - no, it's not feature engineering because you don't create a new feature\n",
    "    - and no, I wouldn't use values from a year ago to fill in the dataset\n",
    "    - generally I'm not a big fan of imputation\n",
    "    - I'll cover one state of the art imputation technique that I'm OK with if used properly\n",
    "    - and I'll show two methods to handle missing values without imputation\n",
    "- **How does training a machine learning model before doing feature selection help in identifying feature interactions?**\n",
    "    - we will learn about feature importance metrics that can measure how important feature pairs are at predicting the target variable\n",
    "- **Do data reduction techniques fall under the category of feature engineering?**\n",
    "    - no, that's feature selection\n",
    "- **Is multivariate imputation only used for continuous variables?**\n",
    "    - yep\n",
    "    - you can preprocess missing values in categorical and ordinal features easily without imputation\n",
    "- **Do we have to plot the relationships between every combination of two features to make sure we did not miss any feature interactions?**\n",
    "    - yes, alternatively see PS5, problem 3\n",
    "- **Will we need to do feature selection in our projects?**\n",
    "    - up to you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluation metrics in supervised ML, part 1, classification\n",
    "By the end of this lecture, you will be able to\n",
    "- Describe the terms in the confusion matrix\n",
    "- Summarize and compare derived metrics (e.g., accuracy, recall, precision, f score)\n",
    "- Choose a metric most appropriate for your problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The supervised ML pipeline\n",
    "The goal: Use the training data (X and y) to develop a <font color='red'>model</font> which can <font color='red'>accurately</font> predict the target variable (y_new') for previously unseen data (X_new).\n",
    "\n",
    "**1. Exploratory Data Analysis (EDA)**: you need to understand your data and verify that it doesn't contain errors\n",
    "   - do as much EDA as you can!\n",
    "    \n",
    "**2. Split the data into different sets**: most often the sets are train, validation, and test (or holdout)\n",
    "   - practitioners often make errors in this step!\n",
    "   - you can split the data randomly, based on groups, based on time, or any other non-standard way if necessary to answer your ML question\n",
    "\n",
    "**3. Preprocess the data**: ML models only work if X and Y are numbers! Some ML models additionally require each feature to have 0 mean and 1 standard deviation (standardized features)\n",
    "   - often the original features you get contain strings (for example a gender feature would contain 'male', 'female', 'non-binary', 'unknown') which needs to transformed into numbers\n",
    "   - often the features are not standardized (e.g., age is between 0 and 100) but it needs to be standardized\n",
    "    \n",
    "<span style=\"background-color: #FFFF00\">**4. Choose an evaluation metric**: depends on the priorities of the stakeholders</span>\n",
    "   - often requires quite a bit of thinking and ethical considerations\n",
    "     \n",
    "**5. Choose one or more ML techniques**: it is highly recommended that you try multiple models\n",
    "   - start with simple models like linear or logistic regression\n",
    "   - try also more complex models like nearest neighbors, support vector machines, random forest, etc.\n",
    "    \n",
    "**6. Tune the hyperparameters of your ML models (aka cross-validation)**\n",
    "   - ML techniques have hyperparameters that you need to optimize to achieve best performance\n",
    "   - for each ML model, decide which parameters to tune and what values to try\n",
    "   - loop through each parameter combination\n",
    "       - train one model for each parameter combination\n",
    "       - evaluate how well the model performs on the validation set\n",
    "   - take the parameter combo that gives the best validation score\n",
    "   - evaluate that model on the test set to report how well the model is expected to perform on previously unseen data\n",
    "    \n",
    "**7. Interpret your model**: black boxes are often not useful\n",
    "   - check if your model uses features that make sense (excellent tool for debugging)\n",
    "   - often model predictions are not enough, you need to be able to explain how the model arrived to a particular prediction (e.g., in health care)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Let's start\n",
    "- decide what metric we will use to evaluate the supervised ML model \n",
    "   - this is necessary even before we train the model\n",
    "   - we need to know what single number metric we will use to compare models and to select the best one\n",
    "- sklearn classifiers have two methods to return predictions\n",
    "   - .predict_proba which returns the probability that the point belongs to each class with shape (n_samples, n_classes)\n",
    "   - .predict which returns the predicted class for each point with shape (n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### .predict_proba vs. .predict\n",
    "\n",
    "`y_true = [1 0 1 1 0] # the true labels`\n",
    "\n",
    "`pred_probs = \n",
    "[[0.02796171 0.97203829]\n",
    " [0.89682444 0.10317556]\n",
    " [0.50104129 0.49895871]\n",
    " [0.13713222 0.86286778]\n",
    " [0.95707434 0.04292566]] # predicted probabilities show the model's confidence`\n",
    " \n",
    " `y_pred = [1 0 0 1 0] # predicted class`\n",
    " - pred_probs\n",
    "    - first column is the probability that the point belongs to class 0\n",
    "    - second column is the probability that the point belings to class 1\n",
    "    - the rows sum to 1\n",
    " - y_pred\n",
    "    - 0 if class 0 probability is equal or larger than 50% (or equivalently if class 1 probability is less than 50%)\n",
    "    - 1 if class 0 probability is less than 50% (or equivalently of class 1 probability is equal or larger than 50%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How to transform predicted probabilities to predicted class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 1 1 0 1 0 1]\n",
      "[0 1 1 0 0 1 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "y_true = np.array([0,0,1,0,1,1,0,1,0,1]) # the true classification labels of the dataset\n",
    "# pred_probs_class1 is the second column of pred_probs\n",
    "pred_probs_class1 = np.array([0.3, 0.7,  0.55, 0.12, 0.45, 0.89, 0.41, 0.02, 0.29, 0.85])\n",
    "p_crit =  0.5\n",
    "\n",
    "# If predicted probability is < p_crit (by default 0.5), predicted class is 0, otherwise it is 1.\n",
    "y_pred = np.zeros(len(pred_probs_class1),dtype=int)\n",
    "y_pred[pred_probs_class1 < p_crit] = 0\n",
    "y_pred[pred_probs_class1 >= p_crit] = 1\n",
    "\n",
    "print(y_true)\n",
    "print(y_pred) # the predicted classification labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For now, we focus on evaluation metrics applicable to predicted classes!\n",
    "\n",
    "We work with y_true and y_pred arrays.\n",
    "\n",
    "Next, we will work with metrics applicable to pred_probs and regression problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color='LIGHTGRAY'>Evaluation metrics in supervised ML, part 1, classification</font>\n",
    "<font color='LIGHTGRAY'>By the end of this lecture, you will be able to</font>\n",
    "- **Describe the terms in the confusion matrix**\n",
    "- <font color='LIGHTGRAY'>Summarize and compare derived metrics (e.g., accuracy, recall, precision, f score)</font>\n",
    "- <font color='LIGHTGRAY'>Choose a metric most appropriate for your problem</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The confusion matrix\n",
    "\n",
    "`y_true = [0, 0, 1, 0, 1, 1, 0, 1, 0, 1] # the true classification labels of the dataset`\n",
    "\n",
    "`y_pred = [0, 1, 1, 0, 0, 1, 0, 0, 0, 1] # the predicted classification labels`\n",
    "\n",
    "Let's count how many points we have in four categories:\n",
    "\n",
    "- true label is 0, predicted label is 0 - **True Negatives**\n",
    "- true label is 1, predicted label is 1 - **True Positives**\n",
    "- true label is 0, predicted label is 1 - **False Positive**\n",
    "- true label is 1, predicted label is 0 - **False Negative**\n",
    "\n",
    "Generally, the confusion matrix $C$ is such that $C_{i,j}$ is equal to the number of observations known to be in group $i$ but predicted to be in group $j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Back to our example:\n",
    "\n",
    "`y_true = [0, 0, 1, 0, 1, 1, 0, 1, 0, 1] # the true classification labels of the dataset`\n",
    "\n",
    "`y_pred = [0, 1, 1, 0, 0, 1, 0, 0, 0, 1] # the predicted classification labels`\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td colspan=\"2\" rowspan=\"2\"></td>\n",
    "        <td colspan=\"2\">Predicted class</td>\t\t\t\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Predicted Negative (0)</td>\n",
    "        <td>Predicted Positive (1)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td rowspan=\"2\">Actual class</td>\n",
    "        <td>Condition Negative (0)</td>\n",
    "        <td><b>True Negative (TN): 4</b></td>\n",
    "        <td><b>False Positive (FP): 1</b></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Condition Positive (1)</td>\n",
    "        <td><b>False Negative (FN): 2</b></td>\n",
    "        <td><b>True Positive (TP): 3</b></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## In sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4 1]\n",
      " [2 3]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_true = [0,0,1,0,1,1,0,1,0,1]\n",
    "y_pred = [0,1,1,0,0,1,0,0,0,1]\n",
    "print(confusion_matrix(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay\n",
    "# check out also https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y_true,y_pred)\n",
    "disp = ConfusionMatrixDisplay(cm,display_labels=['class 0', 'class 1'])\n",
    "disp.plot()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quiz 1\n",
    "\n",
    "Calculate what fraction of the data points are correctly classified in the example below. Visualize the confusion matrix (not part of the quiz)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([0,0,2,1,1,0,2,2,2,0,1,1,0,0,0,1])\n",
    "y_pred = np.array([0,1,0,1,0,0,2,2,1,0,1,1,0,0,1,2])\n",
    "\n",
    "# add you code below\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color='LIGHTGRAY'>Evaluation metrics in supervised ML, part 1, classification</font>\n",
    "<font color='LIGHTGRAY'>By the end of this lecture, you will be able to</font>\n",
    "- <font color='LIGHTGRAY'>Describe the terms in the confusion matrix</font>\n",
    "- **Summarize and compare derived metrics (e.g., accuracy, recall, precision, f score)**\n",
    "- <font color='LIGHTGRAY'>Choose a metric most appropriate for your problem</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Metrics derived from $C$\n",
    "$C$ contains $n_{classes}^2$ elements but we need a single number metric to easily compare various models.\n",
    "\n",
    "For two classes:\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td colspan=\"2\" rowspan=\"2\"></td>\n",
    "        <td colspan=\"2\">Predicted class</td>\t\t\t\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Predicted Negative (0)</td>\n",
    "        <td>Predicted Positive (1)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td rowspan=\"2\">Actual class</td>\n",
    "        <td>Condition Negative (0)</td>\n",
    "        <td><b>True Negative (TN)</b></td>\n",
    "        <td><b>False Positive (FP)</b></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Condition Positive (1)</td>\n",
    "        <td><b>False Negative (FN)</b></td>\n",
    "        <td><b>True Positive (TP)</b></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "Some single number metrics derived from $C$:\n",
    "- accuracy: fraction of data points correctly classified\n",
    "   - $a = \\sum_i C_{i,i} / \\sum C$ = (TP + TN) / (TP + TN + FP + FN)\n",
    "- recall: what fraction of the condition positive samples are true positives?\n",
    "   - it measures the ability of the classifier to identify all positive samples\n",
    "   - in binary classification: R = TP / (TP + FN)\n",
    "- precision: what fraction of the predicted positive points are true positives?\n",
    "   - it measures the ability of the classifier to not predict a negative sample to be positive\n",
    "   - in binary classification: P = TP / (TP + FP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td colspan=\"2\" rowspan=\"2\"></td>\n",
    "        <td colspan=\"2\">Predicted class</td>\t\t\t\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Predicted Negative (0)</td>\n",
    "        <td>Predicted Positive (1)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td rowspan=\"2\">Actual class</td>\n",
    "        <td>Condition Negative (0)</td>\n",
    "        <td><b>True Negative (TN)</b></td>\n",
    "        <td><b>False Positive (FP)</b></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Condition Positive (1)</td>\n",
    "        <td><b>False Negative (FN)</b></td>\n",
    "        <td><b>True Positive (TP)</b></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "A = (TP + TN) / (TP + TN + FP + FN) \n",
    "\n",
    "R = TP / (TP + FN) = TP / CP\n",
    "\n",
    "P = TP / (TP + FP) = TP / PP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The f_beta score\n",
    "Weighted harmonic mean of P and R:\n",
    "### <center> $f_{\\beta} = (1 + \\beta^2) \\frac{P R}{\\beta^2 P + R}$ </center>\n",
    "\n",
    "If $\\beta = 1$, we have the f1 score:\n",
    "### <center> $f_{1} = 2 \\frac{P R}{P + R}$ </center>\n",
    "\n",
    "If $\\beta < 1$, more weight to precision.\n",
    "\n",
    "If $\\beta > 1$, more weight to recall.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The scores are a function of p_crit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 1 1 0 1 0 1]\n",
      "[0 1 1 0 0 1 0 0 0 1]\n",
      "accuracy 0.7\n",
      "recall 0.6\n",
      "precision 0.75\n",
      "f1 0.6666666666666665\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, fbeta_score\n",
    "\n",
    "y_true = np.array([0,0,1,0,1,1,0,1,0,1]) # the true classification labels of the dataset\n",
    "y_pred_proba = np.array([0.3, 0.7,  0.55, 0.12, 0.45, 0.89, 0.41, 0.02, 0.29, 0.85])\n",
    "\n",
    "p_crit = 0.5\n",
    "\n",
    "y_pred = np.zeros(len(y_pred_proba),dtype=int)\n",
    "y_pred[y_pred_proba < p_crit] = 0\n",
    "y_pred[y_pred_proba >= p_crit] = 1\n",
    "\n",
    "print(y_true)\n",
    "print(y_pred) # the predicted classification labels\n",
    "print('accuracy',accuracy_score(y_true,y_pred))\n",
    "print('recall',recall_score(y_true,y_pred))\n",
    "print('precision',precision_score(y_true,y_pred))\n",
    "print('f1',fbeta_score(y_true,y_pred,beta=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quiz 2\n",
    "Given the true and predicted labels, what are the accuracy, recall, precision, and f1 scores? \n",
    "\n",
    "Do not use sklearn to answer the question! First construct the confusion matrix and then calculate the scores by hand!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [0,0,0,1,1,1,0,0]\n",
    "y_pred = [0,1,0,1,1,0,0,0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color='LIGHTGRAY'>Evaluation metrics in supervised ML, part 1, classification</font>\n",
    "<font color='LIGHTGRAY'>By the end of this lecture, you will be able to</font>\n",
    "- <font color='LIGHTGRAY'>Describe the terms in the confusion matrix</font>\n",
    "- <font color='LIGHTGRAY'>Summarize and compare derived metrics (e.g., accuracy, recall, precision, f score)</font>\n",
    "- **Choose a metric most appropriate for your problem**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How should you choose a metric?\n",
    "\n",
    "- What are the terms in the confusion matrix that you most (or least) care about?\n",
    "    - In an imbalanced dataset, TNs are large so you should use a metric that doesn't include TN\n",
    "    - no accuracy\n",
    "    - f score is usually preferred if your dataset is imbalanced\n",
    "- Will we act (intervene/apply treatment) on the model's prediction?\n",
    "    - Is it cheap to act? (e.g., mass email)\n",
    "       - we want to capture the largest fraction of the condition positive samples even if FPs will be large as a result\n",
    "       - recall or fbeta with beta > 1 (f1.5 or f2 are often used)\n",
    "    - Is it expensive to act? Do we have limited resources? Or treatment/action is costly?\n",
    "       - we want to make sure that the resources are allocated the best way possible\n",
    "       - want to make sure that a large fraction of the predicted positives are  true positives\n",
    "       - precision or fbeta with beta < 1 (f0.5 is often used)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mudcard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
